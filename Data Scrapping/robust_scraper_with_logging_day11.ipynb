{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7da2afba-f499-4e6e-ae70-f5c39d7a1b30",
   "metadata": {},
   "source": [
    "***Logging***<br>\n",
    "->Logging is like keeping a diary or a record of what your program is doing.<br>\n",
    "It helps you understand your program, especially when something goes wrong (errors).<br>\n",
    "Why we use it:<br>\n",
    "-To track the flow of a program.<br>\n",
    "-To debug errors.<br>\n",
    "-To keep a history of important events<br>\n",
    "\n",
    "***Automation***<br>\n",
    "->Automation is when you make your computer do repetitive tasks by itself, without you doing it manually.<br>\n",
    "Why we use it:<br>\n",
    "-Save time.<br>\n",
    "-Avoid mistakes from manual work.<br>\n",
    "-Run tasks regularly.<br>\n",
    "\n",
    "***Error-proof scraping design***<br>\n",
    "When we scrape data from websites, many things can go wrong:<br>\n",
    "-Website structure may change.<br>\n",
    "-Some pages may be missing or return errors.<br>\n",
    "-Internet connection may fail.<br>\n",
    "-Some data may be empty or unexpected<br>.\n",
    "\n",
    "Error-proof scraping means designing your scraper so it handles these errors gracefully without crashing and still collects as much data as possible.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "667e697d-26d4-48aa-b169-83b1e741baf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Status code: 200 - https://httpbin.org/get\n",
      "Connection error: Could not reach https://httpstat.us/404\n",
      "Timeout error: https://httpbin.org/delay/10 took too long to respond\n"
     ]
    }
   ],
   "source": [
    "#Add try–except blocks around all network requests to handle common errors such as ConnectionError, Timeout, and unexpected exceptions.\n",
    "import requests\n",
    "\n",
    "\n",
    "urls = [\n",
    "    \"https://httpbin.org/get\",         # should succeed (returns JSON)\n",
    "    \"https://httpstat.us/404\",         # will return a 404 error\n",
    "    \"https://httpbin.org/delay/10\"     # will timeout (delay > timeout)\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)  # timeout after 5 seconds\n",
    "        print(f\"Success! Status code: {response.status_code} - {url}\")\n",
    "\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"Connection error: Could not reach {url}\")\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Timeout error: {url} took too long to respond\")\n",
    "\n",
    "    except requests.exceptions.RequestException:\n",
    "        print(f\"Some other error occurred with {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1553c7d7-c80b-4add-a2c4-2da005e51639",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Configure Python logging:\n",
    "• Create a logger that writes logs to a file (scraper.log)\n",
    "• Log INFO messages for normal flow (page fetched, items parsed)\n",
    "• Log ERROR messages when exceptions occur'''\n",
    "\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "# Configure logger\n",
    "logging.basicConfig(\n",
    "    filename=\"scraper.log\",      # log file name\n",
    "    level=logging.INFO,           # capture INFO and higher level messages\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"  # log format\n",
    ")\n",
    "\n",
    "# List of real URLs\n",
    "urls = [\n",
    "    \"https://httpbin.org/get\",\n",
    "    \"https://httpstat.us/404\",\n",
    "    \"https://httpbin.org/delay/10\"\n",
    "]\n",
    "\n",
    "# Loop through URLs\n",
    "for url in urls:\n",
    "    try:\n",
    "        logging.info(f\"Fetching URL: {url}\")  # INFO message\n",
    "        response = requests.get(url, timeout=5)\n",
    "        logging.info(f\"Success! Status code: {response.status_code} - {url}\") # INFO message\n",
    "        # Example: parsing items (we just simulate here)\n",
    "        items = [\"item1\", \"item2\"]  \n",
    "        logging.info(f\"Parsed items: {items}\")  # INFO message\n",
    "\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        logging.error(f\"Connection error: Could not reach {url}\")  # ERROR message\n",
    "    except requests.exceptions.Timeout:\n",
    "        logging.error(f\"Timeout error: {url} took too long to respond\")  # ERROR message\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Some other error occurred with {url}: {e}\")  # ERROR message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16bda6fb-84d1-4943-9f1b-477e356377f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Implement a retry mechanism for failed requests:\n",
    "• Retry a request up to N times (for example, 3 retries)\n",
    "• Add a small delay between retries\n",
    "• Stop retrying after max attempts and log the failure'''\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename=\"scraper.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "url = \"https://httpbin.org/delay/10\"  # slow URL to force failure\n",
    "max_retries = 3\n",
    "delay_seconds = 2\n",
    "\n",
    "for attempt in range(1, max_retries + 1):\n",
    "    try:\n",
    "        logging.info(f\"Attempt {attempt}: Fetching {url}\")\n",
    "        response = requests.get(url, timeout=3)\n",
    "        response.raise_for_status()\n",
    "        logging.info(\"Page fetched successfully\")\n",
    "        break   # stop retrying if success\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Attempt {attempt} failed: {e}\")\n",
    "        time.sleep(delay_seconds)\n",
    "\n",
    "else:\n",
    "    logging.error(f\"Failed to fetch {url} after {max_retries} attempts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b80217fd-d4c5-43cd-9230-78224d32769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Modify fetch_page() to:\n",
    "• Log the page number being scraped\n",
    "• Log success when status code is 200\n",
    "• Log warning or error when status code is not 200'''\n",
    "\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "# Configure logger\n",
    "logging.basicConfig(\n",
    "    filename=\"scraper.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "def fetch_page(page_number):\n",
    "    url = f\"https://httpbin.org/status/{200 if page_number % 2 == 0 else 404}\"  \n",
    "    # For demo: even pages = 200, odd pages = 404\n",
    "\n",
    "    logging.info(f\"Scraping page number: {page_number}\")  # Log page number\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            logging.info(f\"Page {page_number} fetched successfully!\")\n",
    "            return response.text\n",
    "        else:\n",
    "            logging.warning(f\"Page {page_number} returned status code: {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error fetching page {page_number}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example: scrape pages 1 to 5\n",
    "for i in range(1, 6):\n",
    "    fetch_page(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0a3720c-dfa1-4421-bd59-94c5b7ee6f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ensure the scraper does not crash on a single failure:\n",
    "• If one page fails, handle it gracefully\n",
    "• Continue or stop based on your retry logic'''\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Configure logger\n",
    "logging.basicConfig(\n",
    "    filename=\"scraper.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "def fetch_page(page_number, max_retries=3, delay_seconds=2):\n",
    "    url = f\"https://httpbin.org/status/{200 if page_number % 2 == 0 else 404}\"  \n",
    "    # For demo: even pages = 200, odd pages = 404\n",
    "\n",
    "    logging.info(f\"Scraping page number: {page_number}\")\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=3)\n",
    "            if response.status_code == 200:\n",
    "                logging.info(f\"Page {page_number} fetched successfully!\")\n",
    "                return response.text\n",
    "            else:\n",
    "                logging.warning(f\"Page {page_number} returned status code: {response.status_code}\")\n",
    "                return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Attempt {attempt} failed for page {page_number}: {e}\")\n",
    "            time.sleep(delay_seconds)\n",
    "    # After all retries fail\n",
    "    logging.error(f\"Failed to fetch page {page_number} after {max_retries} attempts\")\n",
    "    return None\n",
    "\n",
    "# Scrape multiple pages without crashing on a single failure\n",
    "for i in range(1, 6):\n",
    "    fetch_page(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2a9d9d4-d544-4a36-a254-c27792607881",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Add a final summary log at the end of execution:\n",
    "• Total pages attempted\n",
    "• Total pages successfully scraped\n",
    "• Total failures'''\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Configure logger\n",
    "logging.basicConfig(\n",
    "    filename=\"scraper.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# Counters for summary\n",
    "total_attempted = 0\n",
    "total_success = 0\n",
    "total_failed = 0\n",
    "\n",
    "def fetch_page(page_number, max_retries=3, delay_seconds=2):\n",
    "    url = f\"https://httpbin.org/status/{200 if page_number % 2 == 0 else 404}\"  \n",
    "    logging.info(f\"Scraping page number: {page_number}\")\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=3)\n",
    "            if response.status_code == 200:\n",
    "                logging.info(f\"Page {page_number} fetched successfully!\")\n",
    "                return True  # success\n",
    "            else:\n",
    "                logging.warning(f\"Page {page_number} returned status code: {response.status_code}\")\n",
    "                return False  # failed due to status code\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Attempt {attempt} failed for page {page_number}: {e}\")\n",
    "            time.sleep(delay_seconds)\n",
    "    \n",
    "    logging.error(f\"Failed to fetch page {page_number} after {max_retries} attempts\")\n",
    "    return False  # failed after all retries\n",
    "\n",
    "# Scrape multiple pages\n",
    "for i in range(1, 6):\n",
    "    total_attempted += 1\n",
    "    success = fetch_page(i)\n",
    "    if success:\n",
    "        total_success += 1\n",
    "    else:\n",
    "        total_failed += 1\n",
    "\n",
    "# Final summary\n",
    "logging.info(\"--------- Scraping Summary-------- \")\n",
    "logging.info(f\"Total pages attempted: {total_attempted}\")\n",
    "logging.info(f\"Total pages successfully scraped: {total_success}\")\n",
    "logging.info(f\"Total failures: {total_failed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fce0435c-e659-4419-8428-0f21ea25571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''7) Automation check\n",
    "• Wrap the scraper execution inside a main() function\n",
    "• Make sure the script can be scheduled or run automatically without manual intervention'''\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Configure logger\n",
    "logging.basicConfig(\n",
    "    filename=\"scraper.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "def fetch_page(page_number, max_retries=3, delay_seconds=2):\n",
    "    url = f\"https://httpbin.org/status/{200 if page_number % 2 == 0 else 404}\"  \n",
    "    logging.info(f\"Scraping page number: {page_number}\")\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=3)\n",
    "            if response.status_code == 200:\n",
    "                logging.info(f\"Page {page_number} fetched successfully!\")\n",
    "                return True\n",
    "            else:\n",
    "                logging.warning(f\"Page {page_number} returned status code: {response.status_code}\")\n",
    "                return False\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Attempt {attempt} failed for page {page_number}: {e}\")\n",
    "            time.sleep(delay_seconds)\n",
    "    \n",
    "    logging.error(f\"Failed to fetch page {page_number} after {max_retries} attempts\")\n",
    "    return False\n",
    "\n",
    "def main():\n",
    "    # Counters for summary\n",
    "    total_attempted = 0\n",
    "    total_success = 0\n",
    "    total_failed = 0\n",
    "\n",
    "    # Scrape multiple pages\n",
    "    for i in range(1, 6):\n",
    "        total_attempted += 1\n",
    "        success = fetch_page(i)\n",
    "        if success:\n",
    "            total_success += 1\n",
    "        else:\n",
    "            total_failed += 1\n",
    "\n",
    "    # Final summary\n",
    "    logging.info(\"---------Scraping Summary --------\")\n",
    "    logging.info(f\"Total pages attempted: {total_attempted}\")\n",
    "    logging.info(f\"Total pages successfully scraped: {total_success}\")\n",
    "    logging.info(f\"Total failures: {total_failed}\")\n",
    "    \n",
    "# Ensure automation-friendly execution\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089f17ee-bbf8-4857-a832-ff0584e75eda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
