{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f9409fe-56e4-4640-81e9-a139a25dfd18",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "**Entropy** is a measure of **uncertainty or confusion** in data.\n",
    "\n",
    "In simple words, entropy tells us **how difficult it is to predict an outcome**.\n",
    "\n",
    "For example, when tossing a **fair coin**, getting Head or Tail is equally likely, so we are not sure what will happen next. This situation has **high entropy**. But if a coin always gives **Head**, then the result is certain and there is **low entropy (zero entropy)**.\n",
    "\n",
    "In machine learning, if a dataset contains **equal numbers of different classes**, the data is mixed and confusing, so entropy is high. If the dataset contains **only one class**, there is no confusion and entropy is zero.\n",
    "\n",
    "**Conclusion:**\n",
    "Entropy measures **how uncertain or mixed the data is**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728539fe-0f50-43c0-954d-9d86adb46766",
   "metadata": {},
   "source": [
    "### Information Gain\n",
    "\n",
    "**Information Gain** means **how much uncertainty is reduced after splitting the data**.\n",
    "\n",
    "In easy words, Information Gain tells us **how useful a split is** in making data more clear.\n",
    "\n",
    "For example, before a split, if a dataset is very mixed (high entropy), we are confused. After splitting the data using a good feature, the groups become more pure and less mixed, so uncertainty decreases. This reduction in uncertainty is called **Information Gain**.\n",
    "\n",
    "In decision trees, the feature with **highest Information Gain** is chosen because it gives the **best split**.\n",
    "\n",
    "**Conclusion :**\n",
    "Information Gain is the **reduction in entropy after splitting the data**, showing how much confusion is removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c556b8ba-b995-4661-b874-57bc6e0e1d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features (Day, Outlook):\n",
      "[1, 'Sunny'] -> Label: No\n",
      "[2, 'Sunny'] -> Label: No\n",
      "[3, 'Overcast'] -> Label: Yes\n",
      "[4, 'Rain'] -> Label: Yes\n",
      "[5, 'Rain'] -> Label: Yes\n",
      "[6, 'Rain'] -> Label: No\n",
      "[7, 'Overcast'] -> Label: Yes\n",
      "[8, 'Sunny'] -> Label: No\n",
      "[9, 'Sunny'] -> Label: Yes\n",
      "[10, 'Rain'] -> Label: Yes\n",
      "[11, 'Sunny'] -> Label: Yes\n"
     ]
    }
   ],
   "source": [
    "# 1 Create a small toy dataset manually (10–12 rows) with:\n",
    "#• 2–3 features\n",
    "#• binary class label\n",
    "\n",
    "import csv\n",
    "\n",
    "# Lists to store features and labels\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Open the CSV file\n",
    "with open(\"C:/Users/shres/OneDrive/Documents/play_tennis.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)  # skip header\n",
    "\n",
    "    # Take first 11 valid rows (day, outlook, Play)\n",
    "    count = 0\n",
    "    for row in reader:\n",
    "        if count >= 11:\n",
    "            break\n",
    "\n",
    "        # row format:  day, outlook, play\n",
    "        day= int(row[0])\n",
    "        outlook= (row[1])\n",
    "        play = (row[-1])\n",
    "\n",
    "        X.append([day, outlook])\n",
    "        y.append(play)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "# Print the manually loaded dataset\n",
    "print(\"Features (Day, Outlook):\")\n",
    "for i in range(len(X)):\n",
    "    print(X[i], \"-> Label:\", y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d8e2f29-bfa1-4012-bf56-b1d0206aaeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 0.946\n"
     ]
    }
   ],
   "source": [
    "# 2 Write a function entropy(y) that:\n",
    "#• counts class proportions\n",
    "#• returns entropy value\n",
    "\n",
    "import math\n",
    "\n",
    "def entropy(y):\n",
    "    total = len(y)\n",
    "    counts = {}\n",
    "\n",
    "    # count each class\n",
    "    for label in y:\n",
    "        if label in counts:\n",
    "            counts[label] += 1\n",
    "        else:\n",
    "            counts[label] = 1\n",
    "\n",
    "    ent = 0\n",
    "    for label in counts:\n",
    "        p = counts[label] / total\n",
    "        ent -= p * math.log2(p)\n",
    "\n",
    "    return ent\n",
    "\n",
    "\n",
    "\n",
    "print(\"Entropy:\", round(entropy(y), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfffa14c-b84c-4e40-8680-7064323a4397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced labels entropy: 1.0\n",
      "Pure labels entropy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# 3 Test entropy() for:\n",
    "#• perfectly balanced labels\n",
    "#• fully pure labels\n",
    "\n",
    "import math\n",
    "\n",
    "def entropy(y):\n",
    "    total = len(y)\n",
    "    counts = {}\n",
    "\n",
    "    for label in y:\n",
    "        if label in counts:\n",
    "            counts[label] += 1\n",
    "        else:\n",
    "            counts[label] = 1\n",
    "\n",
    "    ent = 0\n",
    "    for label in counts:\n",
    "        p = counts[label] / total\n",
    "        ent -= p * math.log2(p)\n",
    "\n",
    "    return ent\n",
    "\n",
    "\n",
    "# Test 1: Perfectly balanced labels\n",
    "y_balanced = ['Yes', 'No', 'Yes', 'No']\n",
    "print(\"Balanced labels entropy:\", entropy(y_balanced))\n",
    "\n",
    "# Test 2: Fully pure labels\n",
    "y_pure = ['Yes', 'Yes', 'Yes', 'Yes']\n",
    "print(\"Pure labels entropy:\", entropy(y_pure))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b89ebab0-b9a6-4254-a985-a2859051c545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left Group (Day <= 5):\n",
      "[1, 'Sunny'] -> No\n",
      "[2, 'Sunny'] -> No\n",
      "[3, 'Overcast'] -> Yes\n",
      "[4, 'Rain'] -> Yes\n",
      "[5, 'Rain'] -> Yes\n",
      "\n",
      "Right Group (Day > 5):\n",
      "[6, 'Rain'] -> No\n",
      "[7, 'Overcast'] -> Yes\n",
      "[8, 'Sunny'] -> No\n",
      "[9, 'Sunny'] -> Yes\n",
      "[10, 'Rain'] -> Yes\n",
      "[11, 'Sunny'] -> Yes\n"
     ]
    }
   ],
   "source": [
    "# 4 Write a function split_dataset(X, y, feature, value) that splits data into:\n",
    "#• left group (feature <= value)\n",
    "#• right group (feature > value)\n",
    "import csv\n",
    "\n",
    "# Lists to store features and labels\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Open the CSV file\n",
    "with open(\"C:/Users/shres/OneDrive/Documents/play_tennis.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)  # skip header\n",
    "\n",
    "    # Take first 11 valid rows (day, outlook, Play)\n",
    "    count = 0\n",
    "    for row in reader:\n",
    "        if count >= 11:\n",
    "            break\n",
    "\n",
    "        # row format:  day, outlook, play\n",
    "        day= int(row[0])\n",
    "        outlook= (row[1])\n",
    "        play = (row[-1])\n",
    "\n",
    "        X.append([day, outlook])\n",
    "        y.append(play)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "def split_dataset(X, y, feature_index, value):\n",
    "    left_X, left_y = [], []\n",
    "    right_X, right_y = [], []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        if X[i][feature_index] <= value:\n",
    "            left_X.append(X[i])\n",
    "            left_y.append(y[i])\n",
    "        else:\n",
    "            right_X.append(X[i])\n",
    "            right_y.append(y[i])\n",
    "\n",
    "    return (left_X, left_y), (right_X, right_y)\n",
    "\n",
    "# Step 3: Example split on Day <= 5\n",
    "(left_X, left_y), (right_X, right_y) = split_dataset(X, y, feature_index=0, value=5)\n",
    "\n",
    "# Step 4: Print results\n",
    "print(\"Left Group (Day <= 5):\")\n",
    "for i in range(len(left_X)):\n",
    "    print(left_X[i], \"->\", left_y[i])\n",
    "\n",
    "print(\"\\nRight Group (Day > 5):\")\n",
    "for i in range(len(right_X)):\n",
    "    print(right_X[i], \"->\", right_y[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e571b58-4f4d-4434-851e-00152538aa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain for split Day <= 5: 0.003\n"
     ]
    }
   ],
   "source": [
    "# 5 Write a function information_gain(X, y, feature, value) using:\n",
    "#• parent entropy\n",
    "#• weighted child entropies\n",
    "\n",
    "import math\n",
    "import csv\n",
    "\n",
    "# Lists to store features and labels\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Open the CSV file\n",
    "with open(\"C:/Users/shres/OneDrive/Documents/play_tennis.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)  # skip header\n",
    "\n",
    "    # Take first 11 valid rows (day, outlook, Play)\n",
    "    count = 0\n",
    "    for row in reader:\n",
    "        if count >= 11:\n",
    "            break\n",
    "\n",
    "        # row format:  day, outlook, play\n",
    "        day= int(row[0])\n",
    "        outlook= (row[1])\n",
    "        play = (row[-1])\n",
    "\n",
    "        X.append([day, outlook])\n",
    "        y.append(play)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "\n",
    "# Step 2: Entropy function\n",
    "def entropy(y):\n",
    "    total = len(y)\n",
    "    counts = {}\n",
    "    for label in y:\n",
    "        if label in counts:\n",
    "            counts[label] += 1\n",
    "        else:\n",
    "            counts[label] = 1\n",
    "\n",
    "    ent = 0\n",
    "    for label in counts:\n",
    "        p = counts[label] / total\n",
    "        ent -= p * math.log2(p)\n",
    "    return ent\n",
    "\n",
    "\n",
    "# Step 3: Split function (numeric: <= value)\n",
    "def split_dataset(X, y, feature_index, value):\n",
    "    left_X, left_y = [], []\n",
    "    right_X, right_y = [], []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        if X[i][feature_index] <= value:\n",
    "            left_X.append(X[i])\n",
    "            left_y.append(y[i])\n",
    "        else:\n",
    "            right_X.append(X[i])\n",
    "            right_y.append(y[i])\n",
    "    return (left_X, left_y), (right_X, right_y)\n",
    "\n",
    "\n",
    "# Step 4: Information Gain function\n",
    "def information_gain(X, y, feature_index, value):\n",
    "    parent_entropy = entropy(y)\n",
    "    (left_X, left_y), (right_X, right_y) = split_dataset(X, y, feature_index, value)\n",
    "\n",
    "    n = len(y)\n",
    "    weighted_child_entropy = (len(left_y)/n)*entropy(left_y) + (len(right_y)/n)*entropy(right_y)\n",
    "\n",
    "    ig = parent_entropy - weighted_child_entropy\n",
    "    return ig\n",
    "\n",
    "\n",
    "# Step 5: Example - split on Day <= 5\n",
    "ig_day = information_gain(X, y, feature_index=0, value=5)\n",
    "print(\"Information Gain for split Day <= 5:\", round(ig_day, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63ef6983-e300-4d8a-b637-d3ffcc524758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Day <= 1 | Information Gain = 0.113\n",
      "Feature Day <= 2 | Information Gain = 0.245\n",
      "Feature Day <= 3 | Information Gain = 0.079\n",
      "Feature Day <= 4 | Information Gain = 0.025\n",
      "Feature Day <= 5 | Information Gain = 0.003\n",
      "Feature Day <= 6 | Information Gain = 0.048\n",
      "Feature Day <= 7 | Information Gain = 0.016\n",
      "Feature Day <= 8 | Information Gain = 0.09\n",
      "Feature Day <= 9 | Information Gain = 0.045\n",
      "Feature Day <= 10 | Information Gain = 0.015\n",
      "Feature Day <= 11 | Information Gain = 0.0\n",
      "Feature Day <= 12 | Information Gain = 0.01\n",
      "Feature Day <= 13 | Information Gain = 0.113\n",
      "Feature Day <= 14 | Information Gain = 0.0\n",
      "\n",
      "Best Split:\n",
      "Feature: Day\n",
      "Split Condition: <= 2\n",
      "Best Information Gain: 0.245\n"
     ]
    }
   ],
   "source": [
    "# 6 Loop over all features & possible split values and:\n",
    "#• compute information gain\n",
    "#• print best split\n",
    "import csv\n",
    "import math\n",
    "\n",
    "# Load Day, Outlook, Play\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "with open(\"C:/Users/shres/OneDrive/Documents/play_tennis.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)\n",
    "\n",
    "    for row in reader:\n",
    "        day = int(row[0])   \n",
    "        outlook = row[1]\n",
    "        play = row[-1]\n",
    "\n",
    "        X.append([day, outlook])\n",
    "        y.append(play)\n",
    "\n",
    "\n",
    "# Entropy function\n",
    "def entropy(y):\n",
    "    total = len(y)\n",
    "    counts = {}\n",
    "\n",
    "    for label in y:\n",
    "        counts[label] = counts.get(label, 0) + 1\n",
    "\n",
    "    ent = 0\n",
    "    for label in counts:\n",
    "        p = counts[label] / total\n",
    "        ent -= p * math.log2(p)\n",
    "\n",
    "    return ent\n",
    "\n",
    "\n",
    "# Split dataset (numeric)\n",
    "def split_dataset(X, y, feature_index, value):\n",
    "    left_X, left_y = [], []\n",
    "    right_X, right_y = [], []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        if X[i][feature_index] <= value:\n",
    "            left_X.append(X[i])\n",
    "            left_y.append(y[i])\n",
    "        else:\n",
    "            right_X.append(X[i])\n",
    "            right_y.append(y[i])\n",
    "\n",
    "    return left_y, right_y\n",
    "\n",
    "\n",
    "# Information Gain\n",
    "def information_gain(X, y, feature_index, value):\n",
    "    parent_entropy = entropy(y)\n",
    "    left_y, right_y = split_dataset(X, y, feature_index, value)\n",
    "\n",
    "    n = len(y)\n",
    "    weighted_entropy = (len(left_y)/n)*entropy(left_y) + (len(right_y)/n)*entropy(right_y)\n",
    "\n",
    "    return parent_entropy - weighted_entropy\n",
    "\n",
    "\n",
    "\n",
    "# Loop over all features & split values\n",
    "\n",
    "\n",
    "best_ig = -1\n",
    "best_value = None\n",
    "\n",
    "values = sorted(set(row[0] for row in X))  # Day values\n",
    "\n",
    "for value in values:\n",
    "    ig = information_gain(X, y, feature_index=0, value=value)\n",
    "    print(f\"Feature Day <= {value} | Information Gain = {round(ig, 3)}\")\n",
    "\n",
    "    if ig > best_ig:\n",
    "        best_ig = ig\n",
    "        best_value = value\n",
    "\n",
    "\n",
    "print(\"\\nBest Split:\")\n",
    "print(\"Feature: Day\")\n",
    "print(\"Split Condition: <=\", best_value)\n",
    "print(\"Best Information Gain:\", round(best_ig, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f132d51-3788-4cfb-a4d3-36488b8a527c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Feature Index: 0\n",
      "Best Split Value: 2\n",
      "Best Information Gain: 0.245\n"
     ]
    }
   ],
   "source": [
    "# 7 Create a function best_split(X, y) that returns:\n",
    "#• best feature\n",
    "#• best split value\n",
    "#• best info gain\n",
    "\n",
    "import csv\n",
    "import math\n",
    "\n",
    "# Load Day, Outlook, Play\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "with open(\"C:/Users/shres/OneDrive/Documents/play_tennis.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)\n",
    "\n",
    "    for row in reader:\n",
    "        day = int(row[0])   \n",
    "        outlook = row[1]\n",
    "        play = row[-1]\n",
    "\n",
    "        X.append([day, outlook])\n",
    "        y.append(play)\n",
    "\n",
    "\n",
    "# Entropy function\n",
    "def entropy(y):\n",
    "    total = len(y)\n",
    "    counts = {}\n",
    "\n",
    "    for label in y:\n",
    "        counts[label] = counts.get(label, 0) + 1\n",
    "\n",
    "    ent = 0\n",
    "    for label in counts:\n",
    "        p = counts[label] / total\n",
    "        ent -= p * math.log2(p)\n",
    "\n",
    "    return ent\n",
    "\n",
    "\n",
    "# Split dataset (numeric)\n",
    "def split_dataset(X, y, feature_index, value):\n",
    "    left_X, left_y = [], []\n",
    "    right_X, right_y = [], []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        if X[i][feature_index] <= value:\n",
    "            left_X.append(X[i])\n",
    "            left_y.append(y[i])\n",
    "        else:\n",
    "            right_X.append(X[i])\n",
    "            right_y.append(y[i])\n",
    "\n",
    "    return left_y, right_y\n",
    "\n",
    "\n",
    "# Information Gain\n",
    "def information_gain(X, y, feature_index, value):\n",
    "    parent_entropy = entropy(y)\n",
    "    left_y, right_y = split_dataset(X, y, feature_index, value)\n",
    "\n",
    "    n = len(y)\n",
    "    weighted_entropy = (len(left_y)/n)*entropy(left_y) + (len(right_y)/n)*entropy(right_y)\n",
    "\n",
    "    return parent_entropy - weighted_entropy\n",
    "    \n",
    "def best_split(X, y):\n",
    "    best_ig = -1\n",
    "    best_feature = None\n",
    "    best_value = None\n",
    "\n",
    "    num_features = len(X[0])\n",
    "\n",
    "    for feature_index in range(num_features):\n",
    "\n",
    "        # only numeric feature: Day (index 0)\n",
    "        if feature_index == 0:\n",
    "            values = sorted(set(row[feature_index] for row in X))\n",
    "\n",
    "            for value in values:\n",
    "                ig = information_gain(X, y, feature_index, value)\n",
    "\n",
    "                if ig > best_ig:\n",
    "                    best_ig = ig\n",
    "                    best_feature = feature_index\n",
    "                    best_value = value\n",
    "\n",
    "    return best_feature, best_value, best_ig\n",
    "\n",
    "\n",
    "feature, value, ig = best_split(X, y)\n",
    "\n",
    "print(\"Best Feature Index:\", feature)\n",
    "print(\"Best Split Value:\", value)\n",
    "print(\"Best Information Gain:\", round(ig, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7d732d5-dc7e-4400-9f3e-5aaff5250ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature': 0, 'value': 2, 'left': {'label': 'No'}, 'right': {'feature': 0, 'value': 13, 'left': {'label': 'Yes'}, 'right': {'label': 'No'}}}\n"
     ]
    }
   ],
   "source": [
    "# 8 Build a recursive function build_tree(X, y, depth) that:\n",
    "#• finds best split\n",
    "#• creates left + right child nodes\n",
    "\n",
    "import csv\n",
    "import math\n",
    "\n",
    "# Load Day, Outlook, Play\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "with open(\"C:/Users/shres/OneDrive/Documents/play_tennis.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)\n",
    "\n",
    "    for row in reader:\n",
    "        day = int(row[0])   \n",
    "        outlook = row[1]\n",
    "        play = row[-1]\n",
    "\n",
    "        X.append([day, outlook])\n",
    "        y.append(play)\n",
    "\n",
    "\n",
    "# Entropy function\n",
    "def entropy(y):\n",
    "    total = len(y)\n",
    "    counts = {}\n",
    "\n",
    "    for label in y:\n",
    "        counts[label] = counts.get(label, 0) + 1\n",
    "\n",
    "    ent = 0\n",
    "    for label in counts:\n",
    "        p = counts[label] / total\n",
    "        ent -= p * math.log2(p)\n",
    "\n",
    "    return ent\n",
    "\n",
    "\n",
    "# Split dataset (numeric)\n",
    "def split_dataset(X, y, feature_index, value):\n",
    "    left_X, left_y = [], []\n",
    "    right_X, right_y = [], []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        if X[i][feature_index] <= value:\n",
    "            left_X.append(X[i])\n",
    "            left_y.append(y[i])\n",
    "        else:\n",
    "            right_X.append(X[i])\n",
    "            right_y.append(y[i])\n",
    "\n",
    "    return left_y, right_y\n",
    "\n",
    "\n",
    "# Information Gain\n",
    "def information_gain(X, y, feature_index, value):\n",
    "    parent_entropy = entropy(y)\n",
    "    left_y, right_y = split_dataset(X, y, feature_index, value)\n",
    "\n",
    "    n = len(y)\n",
    "    weighted_entropy = (len(left_y)/n)*entropy(left_y) + (len(right_y)/n)*entropy(right_y)\n",
    "\n",
    "    return parent_entropy - weighted_entropy\n",
    "    \n",
    "def best_split(X, y):\n",
    "    best_ig = -1\n",
    "    best_feature = None\n",
    "    best_value = None\n",
    "\n",
    "    num_features = len(X[0])\n",
    "\n",
    "    for feature_index in range(num_features):\n",
    "\n",
    "        # only numeric feature: Day (index 0)\n",
    "        if feature_index == 0:\n",
    "            values = sorted(set(row[feature_index] for row in X))\n",
    "\n",
    "            for value in values:\n",
    "                ig = information_gain(X, y, feature_index, value)\n",
    "\n",
    "                if ig > best_ig:\n",
    "                    best_ig = ig\n",
    "                    best_feature = feature_index\n",
    "                    best_value = value\n",
    "\n",
    "    return best_feature, best_value, best_ig\n",
    "\n",
    "\n",
    "feature, value, ig = best_split(X, y)\n",
    "\n",
    "def build_tree(X, y, depth, max_depth=2):\n",
    "    # Count labels\n",
    "    if y.count(y[0]) == len(y):\n",
    "        return {\"label\": y[0]}  # pure node\n",
    "\n",
    "    if depth == max_depth:\n",
    "        # majority class\n",
    "        label = max(set(y), key=y.count)\n",
    "        return {\"label\": label}\n",
    "\n",
    "    # Find best split (Day only)\n",
    "    best_feature, best_value, best_ig = best_split(X, y)\n",
    "\n",
    "    if best_ig == 0 or best_feature is None:\n",
    "        label = max(set(y), key=y.count)\n",
    "        return {\"label\": label}\n",
    "\n",
    "    # Split data\n",
    "    left_X, left_y = [], []\n",
    "    right_X, right_y = [], []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        if X[i][best_feature] <= best_value:\n",
    "            left_X.append(X[i])\n",
    "            left_y.append(y[i])\n",
    "        else:\n",
    "            right_X.append(X[i])\n",
    "            right_y.append(y[i])\n",
    "\n",
    "    # Build children recursively\n",
    "    left_child = build_tree(left_X, left_y, depth + 1, max_depth)\n",
    "    right_child = build_tree(right_X, right_y, depth + 1, max_depth)\n",
    "\n",
    "    return {\n",
    "        \"feature\": best_feature,\n",
    "        \"value\": best_value,\n",
    "        \"left\": left_child,\n",
    "        \"right\": right_child\n",
    "    }\n",
    "tree = build_tree(X, y, depth=0)\n",
    "print(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f54b5994-974e-470d-8ac1-4f1c9d931713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature': 0, 'value': 2, 'left': {'label': 'No'}, 'right': {'feature': 0, 'value': 13, 'left': {'feature': 0, 'value': 8, 'left': {'label': 'Yes'}, 'right': {'label': 'Yes'}}, 'right': {'label': 'No'}}}\n"
     ]
    }
   ],
   "source": [
    "# 9 Add stopping conditions:\n",
    "#• pure node\n",
    "#• max depth reached\n",
    "#• minimum samples per node\n",
    "\n",
    "def build_tree(X, y, depth=0, max_depth=3, min_samples=2):\n",
    "\n",
    "    # Pure node\n",
    "    if y.count(y[0]) == len(y):\n",
    "        return {\"label\": y[0]}\n",
    "\n",
    "    # Max depth reached\n",
    "    if depth == max_depth:\n",
    "        label = max(set(y), key=y.count)\n",
    "        return {\"label\": label}\n",
    "\n",
    "    # Minimum samples\n",
    "    if len(y) <= min_samples:\n",
    "        label = max(set(y), key=y.count)\n",
    "        return {\"label\": label}\n",
    "\n",
    "    # Find best split\n",
    "    feature, value, ig = best_split(X, y)\n",
    "\n",
    "    # If no useful split\n",
    "    if ig == 0:\n",
    "        label = max(set(y), key=y.count)\n",
    "        return {\"label\": label}\n",
    "\n",
    "    # Split dataset\n",
    "    left_y, right_y = split_dataset(X, y, feature, value)\n",
    "    left_X = [X[i] for i in range(len(X)) if X[i][feature] <= value]\n",
    "    right_X = [X[i] for i in range(len(X)) if X[i][feature] > value]\n",
    "\n",
    "    # Recursive calls\n",
    "    return {\n",
    "        \"feature\": feature,\n",
    "        \"value\": value,\n",
    "        \"left\": build_tree(left_X, left_y, depth+1, max_depth, min_samples),\n",
    "        \"right\": build_tree(right_X, right_y, depth+1, max_depth, min_samples)\n",
    "    }\n",
    "\n",
    "  \n",
    "tree = build_tree(X, y, depth=0)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6cdd63c-9c3b-494b-8eff-c1faa2e27282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Yes\n"
     ]
    }
   ],
   "source": [
    "# 10 Write a function predict(tree, x_test) to classify a single input sample.\n",
    "\n",
    "def predict(tree, x_test):\n",
    "\n",
    "    # If leaf node\n",
    "    if \"label\" in tree:\n",
    "        return tree[\"label\"]\n",
    "\n",
    "    # Get split info\n",
    "    feature = tree[\"feature\"]\n",
    "    value = tree[\"value\"]\n",
    "\n",
    "    # Go left or right\n",
    "    if x_test[feature] <= value:\n",
    "        return predict(tree[\"left\"], x_test)\n",
    "    else:\n",
    "        return predict(tree[\"right\"], x_test)\n",
    "        \n",
    "x_test = [6, \"Sunny\"]   \n",
    "result = predict(tree, x_test)\n",
    "print(\"Prediction:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12eaf775-359a-4d25-b779-e60c1d97456c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 'Sunny'] -> No\n",
      "[5, 'Rain'] -> Yes\n",
      "[10, 'Sunny'] -> Yes\n",
      "[14, 'Overcast'] -> No\n"
     ]
    }
   ],
   "source": [
    "# 11 Extend prediction for multiple samples.\n",
    "def predict_multiple(tree, X_test):\n",
    "    predictions = []\n",
    "\n",
    "    for x in X_test:\n",
    "        result = predict(tree, x)\n",
    "        predictions.append(result)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "X_test = [\n",
    "    [1, \"Sunny\"],\n",
    "    [5, \"Rain\"],\n",
    "    [10, \"Sunny\"],\n",
    "    [14, \"Overcast\"]\n",
    "]\n",
    "\n",
    "results = predict_multiple(tree, X_test)\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    print(X_test[i], \"->\", results[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c1ef585-1a11-437e-80cf-ebb9d8075778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No\n"
     ]
    }
   ],
   "source": [
    "# 12 Add support for categorical features (Yes/No/High/Low).\n",
    "def predict(tree, x_test):\n",
    "\n",
    "    # Leaf node\n",
    "    if \"label\" in tree:\n",
    "        return tree[\"label\"]\n",
    "\n",
    "    feature = tree[\"feature\"]\n",
    "    value = tree[\"value\"]\n",
    "    ftype = tree[\"type\"]\n",
    "\n",
    "    # Numeric feature\n",
    "    if ftype == \"num\":\n",
    "        if x_test[feature] <= value:\n",
    "            return predict(tree[\"left\"], x_test)\n",
    "        else:\n",
    "            return predict(tree[\"right\"], x_test)\n",
    "\n",
    "    # Categorical feature\n",
    "    else:\n",
    "        if x_test[feature] == value:\n",
    "            return predict(tree[\"left\"], x_test)\n",
    "        else:\n",
    "            return predict(tree[\"right\"], x_test)\n",
    "tree = {\n",
    "    \"feature\": 1,      \n",
    "    \"value\": \"Sunny\",\n",
    "    \"type\": \"cat\",\n",
    "    \"left\": {\"label\": \"No\"},\n",
    "    \"right\": {\"label\": \"Yes\"}\n",
    "}  \n",
    "\n",
    "x_test = [5, \"Sunny\"]\n",
    "print(predict(tree, x_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4666463-96a2-4cfc-ab5f-9f4002fe5435",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature': 0, 'value': 2, 'type': 'num', 'left': {'prob': {'No': 1.0}}, 'right': {'feature': 0, 'value': 13, 'type': 'num', 'left': {'feature': 0, 'value': 8, 'type': 'num', 'left': {'prob': {'Yes': 0.6666666666666666, 'No': 0.3333333333333333}}, 'right': {'prob': {'Yes': 1.0}}}, 'right': {'prob': {'No': 1.0}}}}\n",
      "Probabilities: {'Yes': 0.6666666666666666, 'No': 0.3333333333333333}\n",
      "Final Prediction: Yes\n"
     ]
    }
   ],
   "source": [
    "# 13 Modify the tree to store class probability instead of only label.\n",
    "\n",
    "import csv\n",
    "import math\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "with open(\"C:/Users/shres/OneDrive/Documents/play_tennis.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)\n",
    "\n",
    "    for row in reader:\n",
    "        day = int(row[0])          # numeric\n",
    "        outlook = row[1]           # categorical (ignored for split)\n",
    "        play = row[-1]             # Yes / No\n",
    "\n",
    "        X.append([day, outlook])\n",
    "        y.append(play)\n",
    "        \n",
    "# Entropy function\n",
    "def entropy(y):\n",
    "    total = len(y)\n",
    "    counts = {}\n",
    "\n",
    "    # count each class\n",
    "    for label in y:\n",
    "        if label in counts:\n",
    "            counts[label] += 1\n",
    "        else:\n",
    "            counts[label] = 1\n",
    "\n",
    "    ent = 0\n",
    "    for label in counts:\n",
    "        p = counts[label] / total\n",
    "        ent -= p * math.log2(p)\n",
    "\n",
    "    return ent\n",
    "\n",
    "# Class probability function\n",
    "def class_probabilities(y):\n",
    "    total = len(y)\n",
    "    probs = {}\n",
    "\n",
    "    for label in y:\n",
    "        probs[label] = probs.get(label, 0) + 1\n",
    "\n",
    "    for label in probs:\n",
    "        probs[label] = probs[label] / total\n",
    "\n",
    "    return probs\n",
    "\n",
    "# Split datase\n",
    "def split_dataset(X, y, feature, value):\n",
    "    left_X, left_y = [], []\n",
    "    right_X, right_y = [], []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        if X[i][feature] <= value:\n",
    "            left_X.append(X[i])\n",
    "            left_y.append(y[i])\n",
    "        else:\n",
    "            right_X.append(X[i])\n",
    "            right_y.append(y[i])\n",
    "\n",
    "    return left_X, left_y, right_X, right_y\n",
    "\n",
    "# Information Gain\n",
    "def information_gain(X, y, feature, value):\n",
    "    parent_entropy = entropy(y)\n",
    "\n",
    "    left_X, left_y, right_X, right_y = split_dataset(X, y, feature, value)\n",
    "\n",
    "    n = len(y)\n",
    "    weighted_entropy = (\n",
    "        (len(left_y)/n) * entropy(left_y) +\n",
    "        (len(right_y)/n) * entropy(right_y)\n",
    "    )\n",
    "\n",
    "    return parent_entropy - weighted_entropy\n",
    "\n",
    "# Best split function\n",
    "def best_split(X, y):\n",
    "    best_ig = -1\n",
    "    best_feature = None\n",
    "    best_value = None\n",
    "\n",
    "    values = sorted(set(row[0] for row in X))  # Day only\n",
    "\n",
    "    for value in values:\n",
    "        ig = information_gain(X, y, 0, value)\n",
    "\n",
    "        if ig > best_ig:\n",
    "            best_ig = ig\n",
    "            best_feature = 0\n",
    "            best_value = value\n",
    "\n",
    "    return best_feature, best_value, best_ig\n",
    "\n",
    "# Build Tree (stores probabilities instead of label)\n",
    "def build_tree(X, y, depth=0, max_depth=3, min_samples=2):\n",
    "\n",
    "    # Pure node\n",
    "    if y.count(y[0]) == len(y):\n",
    "        return {\"prob\": class_probabilities(y)}\n",
    "\n",
    "    # Max depth reached\n",
    "    if depth == max_depth:\n",
    "        return {\"prob\": class_probabilities(y)}\n",
    "\n",
    "    # Minimum samples\n",
    "    if len(y) <= min_samples:\n",
    "        return {\"prob\": class_probabilities(y)}\n",
    "\n",
    "    feature, value, ig = best_split(X, y)\n",
    "\n",
    "    # No useful split\n",
    "    if ig == 0:\n",
    "        return {\"prob\": class_probabilities(y)}\n",
    "\n",
    "    left_X, left_y, right_X, right_y = split_dataset(X, y, feature, value)\n",
    "\n",
    "    return {\n",
    "        \"feature\": feature,\n",
    "        \"value\": value,\n",
    "        \"type\": \"num\",\n",
    "        \"left\": build_tree(left_X, left_y, depth+1, max_depth, min_samples),\n",
    "        \"right\": build_tree(right_X, right_y, depth+1, max_depth, min_samples)\n",
    "    }\n",
    "\n",
    "# Predict function (returns probabilities)\n",
    "def predict(tree, x_test):\n",
    "\n",
    "    # Leaf node\n",
    "    if \"prob\" in tree:\n",
    "        return tree[\"prob\"]\n",
    "\n",
    "    feature = tree[\"feature\"]\n",
    "    value = tree[\"value\"]\n",
    "\n",
    "    if x_test[feature] <= value:\n",
    "        return predict(tree[\"left\"], x_test)\n",
    "    else:\n",
    "        return predict(tree[\"right\"], x_test)\n",
    "\n",
    "# Final label from probability\n",
    "def predict_label(tree, x_test):\n",
    "    probs = predict(tree, x_test)\n",
    "    return max(probs, key=probs.get)\n",
    "\n",
    "tree = build_tree(X, y)\n",
    "\n",
    "x_test = [5, \"Sunny\"]\n",
    "print(tree)\n",
    "print(\"Probabilities:\", predict(tree, x_test))\n",
    "print(\"Final Prediction:\", predict_label(tree, x_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b71981b5-e8aa-449d-a559-1b5368aaad90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 0 <= 2 ?\n",
      "Left ->\n",
      "  {'No': 1.0}\n",
      "Right ->\n",
      "  Feature 0 <= 13 ?\n",
      "  Left ->\n",
      "    Feature 0 <= 8 ?\n",
      "    Left ->\n",
      "      {'Yes': 0.6666666666666666, 'No': 0.3333333333333333}\n",
      "    Right ->\n",
      "      {'Yes': 1.0}\n",
      "  Right ->\n",
      "    {'No': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# 14 Add a function print_tree(tree) to display the structure clearly.\n",
    "def print_tree(tree, depth=0):\n",
    "    space = \"  \" * depth  # indentation\n",
    "\n",
    "    if \"prob\" in tree:  # Leaf node\n",
    "        print(space + str(tree[\"prob\"]))\n",
    "        return\n",
    "\n",
    "    # Internal node\n",
    "    print(space + f\"Feature {tree['feature']} <= {tree['value']} ?\")\n",
    "    print(space + \"Left ->\")\n",
    "    print_tree(tree[\"left\"], depth + 1)\n",
    "    print(space + \"Right ->\")\n",
    "    print_tree(tree[\"right\"], depth + 1)\n",
    "\n",
    "\n",
    "print_tree(tree)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47d98f41-d647-4591-8805-bc6c42563356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 0 <= 2 ?\n",
      "Left ->\n",
      "  {'No': 1.0}\n",
      "Right ->\n",
      "  Feature 0 <= 13 ?\n",
      "  Left ->\n",
      "    Feature 0 <= 8 ?\n",
      "    Left ->\n",
      "      {'Yes': 0.6666666666666666, 'No': 0.3333333333333333}\n",
      "    Right ->\n",
      "      {'Yes': 1.0}\n",
      "  Right ->\n",
      "    {'No': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# 15 Implement pruning rule:\n",
    "#• stop splitting if gain < threshold\n",
    "def build_tree(X, y, depth=0, max_depth=3, min_samples=2, gain_threshold=0.01):\n",
    "    # 1. Pure node\n",
    "    if y.count(y[0]) == len(y):\n",
    "        return {\"prob\": class_probabilities(y)}\n",
    "\n",
    "    # 2. Max depth reached\n",
    "    if depth == max_depth:\n",
    "        return {\"prob\": class_probabilities(y)}\n",
    "\n",
    "    # 3. Minimum samples\n",
    "    if len(y) <= min_samples:\n",
    "        return {\"prob\": class_probabilities(y)}\n",
    "\n",
    "    # 4. Find best split\n",
    "    feature, value, ig = best_split(X, y)\n",
    "\n",
    "    # 5. Pruning rule: stop if gain is too small\n",
    "    if ig < gain_threshold:\n",
    "        return {\"prob\": class_probabilities(y)}\n",
    "\n",
    "    # 6. Split dataset\n",
    "    left_X, left_y, right_X, right_y = split_dataset(X, y, feature, value)\n",
    "\n",
    "    # 7. Recursively build left and right subtrees\n",
    "    return {\n",
    "        \"feature\": feature,\n",
    "        \"value\": value,\n",
    "        \"type\": \"num\",\n",
    "        \"left\": build_tree(left_X, left_y, depth+1, max_depth, min_samples, gain_threshold),\n",
    "        \"right\": build_tree(right_X, right_y, depth+1, max_depth, min_samples, gain_threshold)\n",
    "    }\n",
    "tree = build_tree(X, y, max_depth=3, gain_threshold=0.05)\n",
    "print_tree(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c675b74-10ec-4926-9ca8-3ed52c99a868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without pruning: 0.2\n",
      "Accuracy with pruning: 0.2\n",
      "\n",
      "Tree without pruning:\n",
      "Feature 0 <= 2 ?\n",
      "Left ->\n",
      "  {'No': 1.0}\n",
      "Right ->\n",
      "  Feature 0 <= 11 ?\n",
      "  Left ->\n",
      "    {'Yes': 1.0}\n",
      "  Right ->\n",
      "    {'No': 1.0}\n",
      "\n",
      "Tree with pruning:\n",
      "Feature 0 <= 2 ?\n",
      "Left ->\n",
      "  {'No': 1.0}\n",
      "Right ->\n",
      "  Feature 0 <= 11 ?\n",
      "  Left ->\n",
      "    {'Yes': 1.0}\n",
      "  Right ->\n",
      "    {'No': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# 16 Compare performance:\n",
    "#• train/test split\n",
    "#• compute accuracy manually\n",
    "#• compare with and without pruning\n",
    "import csv\n",
    "import random\n",
    "import math\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "with open(\"C:/Users/shres/OneDrive/Documents/play_tennis.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)\n",
    "    for row in reader:\n",
    "        day = int(row[0])  # numeric\n",
    "        outlook = row[1]   # categorical (ignored for now)\n",
    "        play = row[-1]\n",
    "        X.append([day, outlook])\n",
    "        y.append(play)\n",
    "\n",
    "\n",
    "# Step 1: Train/Test Split\n",
    "\n",
    "def train_test_split(X, y, test_size=0.3):\n",
    "    combined = list(zip(X, y))\n",
    "    random.shuffle(combined)\n",
    "    split_idx = int(len(X)*(1-test_size))\n",
    "    X_train, y_train = zip(*combined[:split_idx])\n",
    "    X_test, y_test = zip(*combined[split_idx:])\n",
    "    return list(X_train), list(y_train), list(X_test), list(y_test)\n",
    "\n",
    "X_train, y_train, X_test, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "\n",
    "# Step 2: Helper Functions\n",
    "\n",
    "# Entropy\n",
    "def entropy(y):\n",
    "    total = len(y)\n",
    "    counts = {}\n",
    "    for label in y:\n",
    "        counts[label] = counts.get(label, 0) + 1\n",
    "    ent = 0\n",
    "    for label in counts:\n",
    "        p = counts[label]/total\n",
    "        ent -= p*math.log2(p)\n",
    "    return ent\n",
    "\n",
    "# Class probabilities\n",
    "def class_probabilities(y):\n",
    "    total = len(y)\n",
    "    counts = {}\n",
    "    for label in y:\n",
    "        counts[label] = counts.get(label,0)+1\n",
    "    for k in counts:\n",
    "        counts[k] /= total\n",
    "    return counts\n",
    "\n",
    "# Split dataset\n",
    "def split_dataset(X, y, feature, value):\n",
    "    left_X, left_y, right_X, right_y = [], [], [], []\n",
    "    for i in range(len(X)):\n",
    "        if X[i][feature] <= value:\n",
    "            left_X.append(X[i])\n",
    "            left_y.append(y[i])\n",
    "        else:\n",
    "            right_X.append(X[i])\n",
    "            right_y.append(y[i])\n",
    "    return left_X, left_y, right_X, right_y\n",
    "\n",
    "# Information gain\n",
    "def information_gain(X, y, feature, value):\n",
    "    parent_entropy = entropy(y)\n",
    "    left_y, right_y = split_dataset(X, y, feature, value)[1::2]\n",
    "    n = len(y)\n",
    "    weighted_entropy = (len(left_y)/n)*entropy(left_y) + (len(right_y)/n)*entropy(right_y)\n",
    "    return parent_entropy - weighted_entropy\n",
    "\n",
    "# Best split\n",
    "def best_split(X, y):\n",
    "    best_ig = -1\n",
    "    best_feature, best_value = None, None\n",
    "    for feature_index in range(len(X[0])):  # only numeric features\n",
    "        if feature_index != 0:  # skip non-numeric\n",
    "            continue\n",
    "        values = sorted(set(row[feature_index] for row in X))\n",
    "        for value in values:\n",
    "            ig = information_gain(X, y, feature_index, value)\n",
    "            if ig > best_ig:\n",
    "                best_ig = ig\n",
    "                best_feature = feature_index\n",
    "                best_value = value\n",
    "    return best_feature, best_value, best_ig\n",
    "\n",
    "\n",
    "# Step 3: Build Tree\n",
    "\n",
    "def build_tree(X, y, depth=0, max_depth=3, min_samples=2, gain_threshold=0.01):\n",
    "    # Pure node\n",
    "    if y.count(y[0]) == len(y):\n",
    "        return {\"prob\": class_probabilities(y)}\n",
    "    # Max depth\n",
    "    if depth == max_depth:\n",
    "        return {\"prob\": class_probabilities(y)}\n",
    "    # Min samples\n",
    "    if len(y) <= min_samples:\n",
    "        return {\"prob\": class_probabilities(y)}\n",
    "    \n",
    "    # Best split\n",
    "    feature, value, ig = best_split(X, y)\n",
    "    # Pruning\n",
    "    if ig < gain_threshold:\n",
    "        return {\"prob\": class_probabilities(y)}\n",
    "    \n",
    "    left_X, left_y, right_X, right_y = split_dataset(X, y, feature, value)\n",
    "    return {\n",
    "        \"feature\": feature,\n",
    "        \"value\": value,\n",
    "        \"type\": \"num\",\n",
    "        \"left\": build_tree(left_X, left_y, depth+1, max_depth, min_samples, gain_threshold),\n",
    "        \"right\": build_tree(right_X, right_y, depth+1, max_depth, min_samples, gain_threshold)\n",
    "    }\n",
    "\n",
    "\n",
    "# Step 4: Prediction\n",
    "\n",
    "def predict_label(tree, x):\n",
    "    if \"prob\" in tree:\n",
    "        return max(tree[\"prob\"], key=tree[\"prob\"].get)\n",
    "    if x[tree[\"feature\"]] <= tree[\"value\"]:\n",
    "        return predict_label(tree[\"left\"], x)\n",
    "    else:\n",
    "        return predict_label(tree[\"right\"], x)\n",
    "\n",
    "def predict(tree, X):\n",
    "    return [predict_label(tree, x) for x in X]\n",
    "\n",
    "\n",
    "# Step 5: Accuracy\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    correct = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == y_pred[i]:\n",
    "            correct += 1\n",
    "    return correct / len(y_true)\n",
    "\n",
    "\n",
    "# Step 6: Train Trees\n",
    "\n",
    "# Tree without pruning\n",
    "tree_no_prune = build_tree(X_train, y_train, gain_threshold=0.0)\n",
    "y_pred_no_prune = predict(tree_no_prune, X_test)\n",
    "acc_no_prune = accuracy(y_test, y_pred_no_prune)\n",
    "\n",
    "# Tree with pruning\n",
    "tree_prune = build_tree(X_train, y_train, gain_threshold=0.05)\n",
    "y_pred_prune = predict(tree_prune, X_test)\n",
    "acc_prune = accuracy(y_test, y_pred_prune)\n",
    "\n",
    "\n",
    "# Step 7: Print Results\n",
    "\n",
    "print(\"Accuracy without pruning:\", round(acc_no_prune,3))\n",
    "print(\"Accuracy with pruning:\", round(acc_prune,3))\n",
    "\n",
    "# Optional: print trees\n",
    "def print_tree(tree, depth=0):\n",
    "    space = \"  \" * depth\n",
    "    if \"prob\" in tree:\n",
    "        print(space + str(tree[\"prob\"]))\n",
    "        return\n",
    "    print(space + f\"Feature {tree['feature']} <= {tree['value']} ?\")\n",
    "    print(space + \"Left ->\")\n",
    "    print_tree(tree[\"left\"], depth + 1)\n",
    "    print(space + \"Right ->\")\n",
    "    print_tree(tree[\"right\"], depth + 1)\n",
    "\n",
    "print(\"\\nTree without pruning:\")\n",
    "print_tree(tree_no_prune)\n",
    "\n",
    "print(\"\\nTree with pruning:\")\n",
    "print_tree(tree_prune)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff79b1e0-a2bc-4658-ab17-07706fcaa8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 0 <= 2 ?\n",
      "Left ->\n",
      "  {'No': 1.0}\n",
      "Right ->\n",
      "  Feature 0 <= 11 ?\n",
      "  Left ->\n",
      "    {'Yes': 1.0}\n",
      "  Right ->\n",
      "    {'No': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# 17 Add max_leaf_nodes stopping condition.\n",
    "\n",
    "# Recursive tree building with max_leaf_nodes\n",
    "def build_tree(X, y, depth=0, max_depth=3, min_samples=2, gain_threshold=0.01, max_leaf_nodes=None, leaf_counter=[0]):\n",
    "    # Pure node\n",
    "    if y.count(y[0]) == len(y):\n",
    "        leaf_counter[0] += 1\n",
    "        return {\"prob\": class_probabilities(y)}\n",
    "    \n",
    "    # Max depth\n",
    "    if depth == max_depth:\n",
    "        leaf_counter[0] += 1\n",
    "        return {\"prob\": class_probabilities(y)}\n",
    "    \n",
    "    # Min samples\n",
    "    if len(y) <= min_samples:\n",
    "        leaf_counter[0] += 1\n",
    "        return {\"prob\": class_probabilities(y)}\n",
    "    \n",
    "    # Max leaf nodes reached\n",
    "    if max_leaf_nodes is not None and leaf_counter[0] >= max_leaf_nodes:\n",
    "        leaf_counter[0] += 1\n",
    "        return {\"prob\": class_probabilities(y)}\n",
    "    \n",
    "    # Best split\n",
    "    feature, value, ig = best_split(X, y)\n",
    "    # Pruning based on info gain\n",
    "    if ig < gain_threshold:\n",
    "        leaf_counter[0] += 1\n",
    "        return {\"prob\": class_probabilities(y)}\n",
    "    \n",
    "    # Split dataset\n",
    "    left_X, left_y, right_X, right_y = split_dataset(X, y, feature, value)\n",
    "    \n",
    "    # Recursively build left and right subtrees\n",
    "    return {\n",
    "        \"feature\": feature,\n",
    "        \"value\": value,\n",
    "        \"type\": \"num\",\n",
    "        \"left\": build_tree(left_X, left_y, depth+1, max_depth, min_samples, gain_threshold, max_leaf_nodes, leaf_counter),\n",
    "        \"right\": build_tree(right_X, right_y, depth+1, max_depth, min_samples, gain_threshold, max_leaf_nodes, leaf_counter)\n",
    "    }\n",
    "    \n",
    "# Reset leaf counter\n",
    "leaf_counter = [0]\n",
    "\n",
    "tree_limited_leaves = build_tree(\n",
    "    X_train, y_train,\n",
    "    max_depth=5,\n",
    "    gain_threshold=0.01,\n",
    "    max_leaf_nodes=3,  # Limit to 3 leaf nodes\n",
    "    leaf_counter=leaf_counter\n",
    ")\n",
    "\n",
    "print_tree(tree_limited_leaves)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6f2bb2f-dcd7-4294-97cf-c8f490b972dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 0 <= 2 ?\n",
      "Left ->\n",
      "  {'No': 1.0}\n",
      "Right ->\n",
      "  Feature 0 <= 11 ?\n",
      "  Left ->\n",
      "    {'Yes': 1.0}\n",
      "  Right ->\n",
      "    {'No': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# 18 Add max_depth stopping condition.\n",
    "\n",
    "# Recursive tree building with max_leaf_nodes\n",
    "def build_tree(X, y, depth=0, max_depth=3, min_samples=2, gain_threshold=0.01, max_leaf_nodes=None, leaf_counter=[0]):\n",
    "    # Pure node\n",
    "    if y.count(y[0]) == len(y):\n",
    "        leaf_counter[0] += 1\n",
    "        return {\"prob\": class_probabilities(y)}\n",
    "    \n",
    "    # Max depth\n",
    "    if depth >= max_depth:\n",
    "        leaf_counter[0] += 1\n",
    "        return {\"prob\": class_probabilities(y)}\n",
    "    \n",
    "    # Min samples\n",
    "    if len(y) <= min_samples:\n",
    "        leaf_counter[0] += 1\n",
    "        return {\"prob\": class_probabilities(y)}\n",
    "    \n",
    "    # Max leaf nodes reached\n",
    "    if max_leaf_nodes is not None and leaf_counter[0] >= max_leaf_nodes:\n",
    "        leaf_counter[0] += 1\n",
    "        return {\"prob\": class_probabilities(y)}\n",
    "    \n",
    "    # Best split\n",
    "    feature, value, ig = best_split(X, y)\n",
    "    # Pruning based on info gain\n",
    "    if ig < gain_threshold:\n",
    "        leaf_counter[0] += 1\n",
    "        return {\"prob\": class_probabilities(y)}\n",
    "    \n",
    "    # Split dataset\n",
    "    left_X, left_y, right_X, right_y = split_dataset(X, y, feature, value)\n",
    "    \n",
    "    # Recursively build left and right subtrees\n",
    "    return {\n",
    "        \"feature\": feature,\n",
    "        \"value\": value,\n",
    "        \"type\": \"num\",\n",
    "        \"left\": build_tree(left_X, left_y, depth+1, max_depth, min_samples, gain_threshold, max_leaf_nodes, leaf_counter),\n",
    "        \"right\": build_tree(right_X, right_y, depth+1, max_depth, min_samples, gain_threshold, max_leaf_nodes, leaf_counter)\n",
    "    }\n",
    "    \n",
    "\n",
    "leaf_counter = [0]\n",
    "tree = build_tree(X_train, y_train, max_depth=2, gain_threshold=0.01, max_leaf_nodes=3, leaf_counter=leaf_counter)\n",
    "\n",
    "print_tree(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29eb43fa-fade-491a-ba7f-8b1395eb374d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best split by Entropy: Feature 0 <= 2 | Info Gain: 0.32\n",
      "Best split by Gini: Feature 0 <= 2 | Gini: 0.283\n"
     ]
    }
   ],
   "source": [
    "# 19 Compute Gini Index and compare with Entropy splitting.\n",
    "import csv\n",
    "\n",
    "# Lists to store features and labels\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Open the CSV file\n",
    "with open(\"C:/Users/shres/OneDrive/Documents/play_tennis.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)  # skip header\n",
    "\n",
    "    # Take first 11 valid rows (day, outlook, Play)\n",
    "    count = 0\n",
    "    for row in reader:\n",
    "        if count >= 11:\n",
    "            break\n",
    "\n",
    "        # row format:  day, outlook, play\n",
    "        day= int(row[0])\n",
    "        outlook= (row[1])\n",
    "        play = (row[-1])\n",
    "\n",
    "        X.append([day, outlook])\n",
    "        y.append(play)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "def gini_index(y):\n",
    "    total = len(y)\n",
    "    counts = {}\n",
    "\n",
    "    # count each class\n",
    "    for label in y:\n",
    "        if label in counts:\n",
    "            counts[label] += 1\n",
    "        else:\n",
    "            counts[label] = 1\n",
    "\n",
    "    gini = 1\n",
    "    for label in counts:\n",
    "        p = counts[label] / total\n",
    "        gini -= p**2  # square of probability\n",
    "\n",
    "    return gini\n",
    "\n",
    "def gini_split(X, y, feature, value):\n",
    "    left_X, left_y, right_X, right_y = split_dataset(X, y, feature, value)\n",
    "    n = len(y)\n",
    "    weighted_gini = (len(left_y)/n)*gini_index(left_y) + (len(right_y)/n)*gini_index(right_y)\n",
    "    return weighted_gini\n",
    "    \n",
    "def best_split_gini(X, y):\n",
    "    best_gini = float('inf')\n",
    "    best_feature, best_value = None, None\n",
    "    for feature_index in range(len(X[0])):  # numeric only\n",
    "        if feature_index != 0: \n",
    "            continue\n",
    "        values = sorted(set(row[feature_index] for row in X))\n",
    "        for value in values:\n",
    "            gini = gini_split(X, y, feature_index, value)\n",
    "            if gini < best_gini:\n",
    "                best_gini = gini\n",
    "                best_feature = feature_index\n",
    "                best_value = value\n",
    "    return best_feature, best_value, best_gini\n",
    "\n",
    "def best_split_entropy(X, y):\n",
    "    best_ig = -1\n",
    "    best_feature, best_value = None, None\n",
    "    for feature_index in range(len(X[0])):\n",
    "        if feature_index != 0: \n",
    "            continue\n",
    "        values = sorted(set(row[feature_index] for row in X))\n",
    "        for value in values:\n",
    "            ig = information_gain(X, y, feature_index, value)\n",
    "            if ig > best_ig:\n",
    "                best_ig = ig\n",
    "                best_feature = feature_index\n",
    "                best_value = value\n",
    "    return best_feature, best_value, best_ig\n",
    "\n",
    "\n",
    "\n",
    "# Best split by Entropy\n",
    "f_e, v_e, ig = best_split_entropy(X, y)\n",
    "print(\"Best split by Entropy: Feature\", f_e, \"<=\", v_e, \"| Info Gain:\", round(ig,3))\n",
    "\n",
    "# Best split by Gini\n",
    "f_g, v_g, g = best_split_gini(X, y)\n",
    "print(\"Best split by Gini: Feature\", f_g, \"<=\", v_g, \"| Gini:\", round(g,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97ee3b4a-5538-4af5-8e52-5ff3990007b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree using Entropy:\n",
      "Feature 0 <= 2 ?\n",
      "Left ->\n",
      "  {'No': 1.0}\n",
      "Right ->\n",
      "  Feature 0 <= 5 ?\n",
      "  Left ->\n",
      "    {'Yes': 1.0}\n",
      "  Right ->\n",
      "    Feature 0 <= 8 ?\n",
      "    Left ->\n",
      "      {'No': 0.6666666666666666, 'Yes': 0.3333333333333333}\n",
      "    Right ->\n",
      "      {'Yes': 1.0}\n",
      "\n",
      "Tree using Gini:\n",
      "Feature 0 <= 2 ?\n",
      "Left ->\n",
      "  {'No': 1.0}\n",
      "Right ->\n",
      "  Feature 0 <= 5 ?\n",
      "  Left ->\n",
      "    {'Yes': 1.0}\n",
      "  Right ->\n",
      "    Feature 0 <= 8 ?\n",
      "    Left ->\n",
      "      {'No': 0.6666666666666666, 'Yes': 0.3333333333333333}\n",
      "    Right ->\n",
      "      {'Yes': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# 20 Allow user to choose between Gini and Entropy.\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "# Entropy\n",
    "\n",
    "def entropy(y):\n",
    "    total = len(y)\n",
    "    counts = {}\n",
    "    for label in y:\n",
    "        counts[label] = counts.get(label, 0) + 1\n",
    "    ent = 0\n",
    "    for label in counts:\n",
    "        p = counts[label] / total\n",
    "        ent -= p * math.log2(p)\n",
    "    return ent\n",
    "\n",
    "\n",
    "# Gini Index\n",
    "\n",
    "def gini_index(y):\n",
    "    total = len(y)\n",
    "    counts = {}\n",
    "    for label in y:\n",
    "        counts[label] = counts.get(label,0)+1\n",
    "    gini = 1\n",
    "    for label in counts:\n",
    "        p = counts[label]/total\n",
    "        gini -= p**2\n",
    "    return gini\n",
    "\n",
    "\n",
    "# Class probabilities\n",
    "\n",
    "def class_probabilities(y):\n",
    "    total = len(y)\n",
    "    counts = {}\n",
    "    for label in y:\n",
    "        counts[label] = counts.get(label,0)+1\n",
    "    for k in counts:\n",
    "        counts[k] /= total\n",
    "    return counts\n",
    "\n",
    "\n",
    "# Split dataset\n",
    "\n",
    "def split_dataset(X, y, feature, value):\n",
    "    left_X, left_y, right_X, right_y = [], [], [], []\n",
    "    for i in range(len(X)):\n",
    "        if X[i][feature] <= value:\n",
    "            left_X.append(X[i])\n",
    "            left_y.append(y[i])\n",
    "        else:\n",
    "            right_X.append(X[i])\n",
    "            right_y.append(y[i])\n",
    "    return left_X, left_y, right_X, right_y\n",
    "\n",
    "\n",
    "# Best split (user chooses criterion)\n",
    "\n",
    "def best_split(X, y, criterion=\"entropy\"):\n",
    "    best_score = -1 if criterion==\"entropy\" else float(\"inf\")\n",
    "    best_feature, best_value = None, None\n",
    "    \n",
    "    for feature_index in range(len(X[0])):  # only numeric\n",
    "        if feature_index != 0:\n",
    "            continue\n",
    "        values = sorted(set(row[feature_index] for row in X))\n",
    "        for value in values:\n",
    "            left_X, left_y, right_X, right_y = split_dataset(X, y, feature_index, value)\n",
    "            n = len(y)\n",
    "            if criterion==\"entropy\":\n",
    "                # Information Gain\n",
    "                weighted_entropy = (len(left_y)/n)*entropy(left_y) + (len(right_y)/n)*entropy(right_y)\n",
    "                score = entropy(y) - weighted_entropy\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_feature = feature_index\n",
    "                    best_value = value\n",
    "            elif criterion==\"gini\":\n",
    "                # Weighted Gini\n",
    "                weighted_gini = (len(left_y)/n)*gini_index(left_y) + (len(right_y)/n)*gini_index(right_y)\n",
    "                score = weighted_gini\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    best_feature = feature_index\n",
    "                    best_value = value\n",
    "    return best_feature, best_value, best_score\n",
    "\n",
    "\n",
    "# Build tree\n",
    "\n",
    "def build_tree(X, y, depth=0, max_depth=3, min_samples=2, gain_threshold=0.01,\n",
    "               max_leaf_nodes=None, leaf_counter=[0], criterion=\"entropy\"):\n",
    "    # Pure node\n",
    "    if y.count(y[0]) == len(y):\n",
    "        leaf_counter[0] += 1\n",
    "        return {\"prob\": class_probabilities(y)}\n",
    "    # Max depth\n",
    "    if depth >= max_depth:\n",
    "        leaf_counter[0] += 1\n",
    "        return {\"prob\": class_probabilities(y)}\n",
    "    # Min samples\n",
    "    if len(y) <= min_samples:\n",
    "        leaf_counter[0] += 1\n",
    "        return {\"prob\": class_probabilities(y)}\n",
    "    # Max leaf nodes\n",
    "    if max_leaf_nodes is not None and leaf_counter[0] >= max_leaf_nodes:\n",
    "        leaf_counter[0] += 1\n",
    "        return {\"prob\": class_probabilities(y)}\n",
    "    \n",
    "    # Best split\n",
    "    feature, value, score = best_split(X, y, criterion)\n",
    "    \n",
    "    # Pruning: stop if gain too small (only for entropy)\n",
    "    if criterion==\"entropy\" and score < gain_threshold:\n",
    "        leaf_counter[0] += 1\n",
    "        return {\"prob\": class_probabilities(y)}\n",
    "    \n",
    "    # Split dataset\n",
    "    left_X, left_y, right_X, right_y = split_dataset(X, y, feature, value)\n",
    "    \n",
    "    return {\n",
    "        \"feature\": feature,\n",
    "        \"value\": value,\n",
    "        \"type\": \"num\",\n",
    "        \"left\": build_tree(left_X, left_y, depth+1, max_depth, min_samples, gain_threshold, max_leaf_nodes, leaf_counter, criterion),\n",
    "        \"right\": build_tree(right_X, right_y, depth+1, max_depth, min_samples, gain_threshold, max_leaf_nodes, leaf_counter, criterion)\n",
    "    }\n",
    "\n",
    "\n",
    "# Predict\n",
    "\n",
    "def predict_label(tree, x):\n",
    "    if \"prob\" in tree:\n",
    "        return max(tree[\"prob\"], key=tree[\"prob\"].get)\n",
    "    if x[tree[\"feature\"]] <= tree[\"value\"]:\n",
    "        return predict_label(tree[\"left\"], x)\n",
    "    else:\n",
    "        return predict_label(tree[\"right\"], x)\n",
    "\n",
    "def predict(tree, X):\n",
    "    return [predict_label(tree, x) for x in X]\n",
    "\n",
    "\n",
    "# Print tree\n",
    "\n",
    "def print_tree(tree, depth=0):\n",
    "    space = \"  \" * depth\n",
    "    if \"prob\" in tree:\n",
    "        print(space + str(tree[\"prob\"]))\n",
    "        return\n",
    "    print(space + f\"Feature {tree['feature']} <= {tree['value']} ?\")\n",
    "    print(space + \"Left ->\")\n",
    "    print_tree(tree[\"left\"], depth + 1)\n",
    "    print(space + \"Right ->\")\n",
    "    print_tree(tree[\"right\"], depth + 1)\n",
    "\n",
    "# Build tree using Entropy\n",
    "leaf_counter = [0]\n",
    "tree_entropy = build_tree(X, y, max_depth=3, criterion=\"entropy\", leaf_counter=leaf_counter)\n",
    "print(\"Tree using Entropy:\")\n",
    "print_tree(tree_entropy)\n",
    "\n",
    "# Build tree using Gini\n",
    "leaf_counter = [0]\n",
    "tree_gini = build_tree(X, y, max_depth=3, criterion=\"gini\", leaf_counter=leaf_counter)\n",
    "print(\"\\nTree using Gini:\")\n",
    "print_tree(tree_gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db7ed6c3-e097-476a-8a6a-4778903666b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left X : [[1], [2], [None]]\n",
      "Left y : ['No', 'No', 'Yes']\n",
      "Right X: [[4], [6]]\n",
      "Right y: ['Yes', 'Yes']\n"
     ]
    }
   ],
   "source": [
    "# 21 Handle missing values when splitting.\n",
    "def split_dataset(X, y, feature, value):\n",
    "    left_X, left_y = [], []\n",
    "    right_X, right_y = [], []\n",
    "    missing_X, missing_y = [], []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        if X[i][feature] is None or X[i][feature] == \"\":\n",
    "            missing_X.append(X[i])\n",
    "            missing_y.append(y[i])\n",
    "        elif X[i][feature] <= value:\n",
    "            left_X.append(X[i])\n",
    "            left_y.append(y[i])\n",
    "        else:\n",
    "            right_X.append(X[i])\n",
    "            right_y.append(y[i])\n",
    "\n",
    "    # Send missing values to the larger side\n",
    "    if len(left_y) >= len(right_y):\n",
    "        left_X.extend(missing_X)\n",
    "        left_y.extend(missing_y)\n",
    "    else:\n",
    "        right_X.extend(missing_X)\n",
    "        right_y.extend(missing_y)\n",
    "\n",
    "    return left_X, left_y, right_X, right_y\n",
    "    \n",
    "X = [\n",
    "    [1],\n",
    "    [2],\n",
    "    [None],   # missing value\n",
    "    [4],\n",
    "    [6]\n",
    "]\n",
    "\n",
    "y = [\"No\", \"No\", \"Yes\", \"Yes\", \"Yes\"]\n",
    "\n",
    "\n",
    "left_X, left_y, right_X, right_y = split_dataset(X, y, feature=0, value=3)\n",
    "\n",
    "print(\"Left X :\", left_X)\n",
    "print(\"Left y :\", left_y)\n",
    "print(\"Right X:\", right_X)\n",
    "print(\"Right y:\", right_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d77aad4-59fb-4558-8f8d-b5dc2bd624de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DECISION TREE STRUCTURE:\n",
      "\n",
      "Feature 0 <= 2 ?\n",
      "Left:\n",
      "  Leaf: {'No': 1.0}\n",
      "Right:\n",
      "  Feature 1 <= Overcast ?\n",
      "  Left:\n",
      "    Leaf: {'Yes': 1.0}\n",
      "  Right:\n",
      "    Feature 1 <= Rain ?\n",
      "    Left:\n",
      "      Leaf: {'Yes': 0.6, 'No': 0.4}\n",
      "    Right:\n",
      "      Leaf: {'No': 0.3333333333333333, 'Yes': 0.6666666666666666}\n",
      "\n",
      "PREDICTIONS:\n",
      "[1, 'Sunny'] => No\n",
      "[2, 'Sunny'] => No\n",
      "[3, 'Overcast'] => Yes\n",
      "[4, 'Rain'] => Yes\n",
      "[5, 'Rain'] => Yes\n",
      "[6, 'Rain'] => Yes\n",
      "[7, 'Overcast'] => Yes\n",
      "[8, 'Sunny'] => Yes\n",
      "[9, 'Sunny'] => Yes\n",
      "[10, 'Rain'] => Yes\n",
      "[11, 'Sunny'] => Yes\n",
      "[12, 'Overcast'] => Yes\n",
      "[13, 'Overcast'] => Yes\n",
      "[14, 'Rain'] => Yes\n"
     ]
    }
   ],
   "source": [
    "# 22 Implement random feature selection (Decision Tree Lite → Random Forest concept intro).\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "with open(\"C:/Users/shres/OneDrive/Documents/play_tennis.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)\n",
    "\n",
    "    for row in reader:\n",
    "        day = int(row[0])        # numeric\n",
    "        outlook = row[1]         # categorical\n",
    "        play = row[-1]           # Yes / No\n",
    "\n",
    "        X.append([day, outlook])\n",
    "        y.append(play)\n",
    "\n",
    "\n",
    "\n",
    "# Entropy\n",
    "\n",
    "def entropy(y):\n",
    "    total = len(y)\n",
    "    counts = {}\n",
    "\n",
    "    for label in y:\n",
    "        counts[label] = counts.get(label, 0) + 1\n",
    "\n",
    "    ent = 0\n",
    "    for label in counts:\n",
    "        p = counts[label] / total\n",
    "        ent -= p * math.log2(p)\n",
    "\n",
    "    return ent\n",
    "\n",
    "\n",
    "\n",
    "# Gini Index\n",
    "\n",
    "def gini(y):\n",
    "    total = len(y)\n",
    "    counts = {}\n",
    "\n",
    "    for label in y:\n",
    "        counts[label] = counts.get(label, 0) + 1\n",
    "\n",
    "    g = 1\n",
    "    for label in counts:\n",
    "        p = counts[label] / total\n",
    "        g -= p * p\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "\n",
    "# Split Dataset\n",
    "\n",
    "def split_dataset(X, y, feature, value):\n",
    "    left_y, right_y = [], []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        if X[i][feature] <= value:\n",
    "            left_y.append(y[i])\n",
    "        else:\n",
    "            right_y.append(y[i])\n",
    "\n",
    "    return left_y, right_y\n",
    "\n",
    "\n",
    "\n",
    "# Information Gain\n",
    "\n",
    "def information_gain(X, y, feature, value, criterion=\"entropy\"):\n",
    "    if criterion == \"entropy\":\n",
    "        parent = entropy(y)\n",
    "        measure = entropy\n",
    "    else:\n",
    "        parent = gini(y)\n",
    "        measure = gini\n",
    "\n",
    "    left_y, right_y = split_dataset(X, y, feature, value)\n",
    "\n",
    "    n = len(y)\n",
    "    weighted = (len(left_y)/n)*measure(left_y) + (len(right_y)/n)*measure(right_y)\n",
    "\n",
    "    return parent - weighted\n",
    "\n",
    "\n",
    "\n",
    "# Random Feature Selection\n",
    "\n",
    "def random_features(num_features):\n",
    "    k = int(math.sqrt(num_features))\n",
    "    if k < 1:\n",
    "        k = 1\n",
    "    return random.sample(range(num_features), k)\n",
    "\n",
    "\n",
    "\n",
    "# Best Split (Random Feature Selection)\n",
    "\n",
    "def best_split(X, y, criterion=\"entropy\"):\n",
    "    best_ig = -1\n",
    "    best_feature = None\n",
    "    best_value = None\n",
    "\n",
    "    num_features = len(X[0])\n",
    "    features_to_try = random_features(num_features)\n",
    "\n",
    "    for feature in features_to_try:\n",
    "        values = sorted(set(row[feature] for row in X))\n",
    "\n",
    "        for value in values:\n",
    "            ig = information_gain(X, y, feature, value, criterion)\n",
    "\n",
    "            if ig > best_ig:\n",
    "                best_ig = ig\n",
    "                best_feature = feature\n",
    "                best_value = value\n",
    "\n",
    "    return best_feature, best_value, best_ig\n",
    "\n",
    "\n",
    "\n",
    "# Class Probability\n",
    "\n",
    "def class_probability(y):\n",
    "    probs = {}\n",
    "    total = len(y)\n",
    "\n",
    "    for label in y:\n",
    "        probs[label] = probs.get(label, 0) + 1\n",
    "\n",
    "    for label in probs:\n",
    "        probs[label] /= total\n",
    "\n",
    "    return probs\n",
    "\n",
    "\n",
    "\n",
    "# Build Tree (Recursive)\n",
    "\n",
    "def build_tree(X, y, depth=0, max_depth=3, min_samples=2, criterion=\"entropy\"):\n",
    "    # stopping conditions\n",
    "    if len(set(y)) == 1:\n",
    "        return {\"prob\": class_probability(y)}\n",
    "\n",
    "    if depth == max_depth or len(y) < min_samples:\n",
    "        return {\"prob\": class_probability(y)}\n",
    "\n",
    "    feature, value, ig = best_split(X, y, criterion)\n",
    "\n",
    "    if ig <= 0:\n",
    "        return {\"prob\": class_probability(y)}\n",
    "\n",
    "    left_X, left_y = [], []\n",
    "    right_X, right_y = [], []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        if X[i][feature] <= value:\n",
    "            left_X.append(X[i])\n",
    "            left_y.append(y[i])\n",
    "        else:\n",
    "            right_X.append(X[i])\n",
    "            right_y.append(y[i])\n",
    "\n",
    "    return {\n",
    "        \"feature\": feature,\n",
    "        \"value\": value,\n",
    "        \"left\": build_tree(left_X, left_y, depth+1, max_depth, min_samples, criterion),\n",
    "        \"right\": build_tree(right_X, right_y, depth+1, max_depth, min_samples, criterion)\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Predict One Sample\n",
    "\n",
    "def predict(tree, x):\n",
    "    if \"prob\" in tree:\n",
    "        return max(tree[\"prob\"], key=tree[\"prob\"].get)\n",
    "\n",
    "    if x[tree[\"feature\"]] <= tree[\"value\"]:\n",
    "        return predict(tree[\"left\"], x)\n",
    "    else:\n",
    "        return predict(tree[\"right\"], x)\n",
    "\n",
    "\n",
    "\n",
    "# Predict Multiple Samples\n",
    "\n",
    "def predict_all(tree, X_test):\n",
    "    return [predict(tree, x) for x in X_test]\n",
    "\n",
    "\n",
    "\n",
    "# Print Tree\n",
    "\n",
    "def print_tree(tree, depth=0):\n",
    "    space = \"  \" * depth\n",
    "\n",
    "    if \"prob\" in tree:\n",
    "        print(space + \"Leaf:\", tree[\"prob\"])\n",
    "        return\n",
    "\n",
    "    print(space + f\"Feature {tree['feature']} <= {tree['value']} ?\")\n",
    "    print(space + \"Left:\")\n",
    "    print_tree(tree[\"left\"], depth+1)\n",
    "    print(space + \"Right:\")\n",
    "    print_tree(tree[\"right\"], depth+1)\n",
    "\n",
    "\n",
    "# Build & Test\n",
    "\n",
    "tree = build_tree(X, y, criterion=\"entropy\")\n",
    "\n",
    "print(\"\\nDECISION TREE STRUCTURE:\\n\")\n",
    "print_tree(tree)\n",
    "\n",
    "print(\"\\nPREDICTIONS:\")\n",
    "preds = predict_all(tree, X)\n",
    "for i in range(len(preds)):\n",
    "    print(X[i], \"=>\", preds[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ff2ad46-4d4e-40a2-bf18-b7a798e4d352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 Accuracy: 0.786\n",
      "Run 2 Accuracy: 0.857\n",
      "Run 3 Accuracy: 0.714\n",
      "Run 4 Accuracy: 0.929\n",
      "Run 5 Accuracy: 0.857\n",
      "\n",
      "Average Accuracy: 0.829\n",
      "All Accuracies: [0.7857142857142857, 0.8571428571428571, 0.7142857142857143, 0.9285714285714286, 0.8571428571428571]\n"
     ]
    }
   ],
   "source": [
    "# 23 Evaluate model stability by training multiple times.\n",
    "# ------------------------------------\n",
    "# Accuracy function\n",
    "# ------------------------------------\n",
    "def accuracy(y_true, y_pred):\n",
    "    correct = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == y_pred[i]:\n",
    "            correct += 1\n",
    "    return correct / len(y_true)\n",
    "\n",
    "\n",
    "# ------------------------------------\n",
    "# Model Stability Evaluation\n",
    "# ------------------------------------\n",
    "def evaluate_stability(X, y, runs=5):\n",
    "    accuracies = []\n",
    "\n",
    "    for i in range(runs):\n",
    "        tree = build_tree(X, y, criterion=\"entropy\")\n",
    "        predictions = predict_all(tree, X)\n",
    "        acc = accuracy(y, predictions)\n",
    "        accuracies.append(acc)\n",
    "\n",
    "        print(f\"Run {i+1} Accuracy: {round(acc, 3)}\")\n",
    "\n",
    "    avg_acc = sum(accuracies) / len(accuracies)\n",
    "\n",
    "    print(\"\\nAverage Accuracy:\", round(avg_acc, 3))\n",
    "    print(\"All Accuracies:\", accuracies)\n",
    "\n",
    "\n",
    "\n",
    "# Run Stability Test\n",
    "\n",
    "evaluate_stability(X, y, runs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d071467-8c6a-4a48-9189-a68983a63e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: [1, 'Sunny']\n",
      "\n",
      "Decision Path:\n",
      "\n",
      "Check: Feature 0 <= 2 ?\n",
      "Yes → go LEFT (value = 1)\n",
      "  Reached leaf\n",
      "  Class probabilities: {'No': 1.0}\n",
      "  Final Prediction: No\n"
     ]
    }
   ],
   "source": [
    "# 24 Create visualization of decision path for one sample.\n",
    "def show_decision_path(tree, x, depth=0):\n",
    "    space = \"  \" * depth\n",
    "\n",
    "    # If leaf node\n",
    "    if \"prob\" in tree:\n",
    "        print(space + \"Reached leaf\")\n",
    "        print(space + \"Class probabilities:\", tree[\"prob\"])\n",
    "        prediction = max(tree[\"prob\"], key=tree[\"prob\"].get)\n",
    "        print(space + \"Final Prediction:\", prediction)\n",
    "        return\n",
    "\n",
    "    feature = tree[\"feature\"]\n",
    "    value = tree[\"value\"]\n",
    "\n",
    "    print(space + f\"Check: Feature {feature} <= {value} ?\")\n",
    "\n",
    "    if x[feature] <= value:\n",
    "        print(space + f\"Yes → go LEFT (value = {x[feature]})\")\n",
    "        show_decision_path(tree[\"left\"], x, depth + 1)\n",
    "    else:\n",
    "        print(space + f\"No → go RIGHT (value = {x[feature]})\")\n",
    "        show_decision_path(tree[\"right\"], x, depth + 1)\n",
    "        \n",
    "sample = X[0]   # take first sample\n",
    "print(\"Sample:\", sample)\n",
    "print(\"\\nDecision Path:\\n\")\n",
    "show_decision_path(tree, sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3099d911-1f6d-4c37-96ac-99df3ef8e859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree saved to play_tennis_tree.json\n",
      "Tree loaded from play_tennis_tree.json\n",
      "\n",
      "Sample: [1, 'Sunny']\n",
      "Prediction from loaded tree: No\n"
     ]
    }
   ],
   "source": [
    "# 25 Save the tree structure to JSON and reload it for prediction.\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "# Save tree to JSON\n",
    "\n",
    "def save_tree(tree, filename=\"tree.json\"):\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(tree, f)\n",
    "    print(f\"Tree saved to {filename}\")\n",
    "\n",
    "\n",
    "\n",
    "# Load tree from JSON\n",
    "\n",
    "def load_tree(filename=\"tree.json\"):\n",
    "    with open(filename, \"r\") as f:\n",
    "        tree = json.load(f)\n",
    "    print(f\"Tree loaded from {filename}\")\n",
    "    return tree\n",
    "    \n",
    "#  Build the tree\n",
    "tree = build_tree(X, y, criterion=\"entropy\")\n",
    "\n",
    "#  Save it\n",
    "save_tree(tree, \"play_tennis_tree.json\")\n",
    "\n",
    "#  Load it later\n",
    "loaded_tree = load_tree(\"play_tennis_tree.json\")\n",
    "\n",
    "#  Predict using loaded tree\n",
    "sample = X[0]\n",
    "prediction = predict(loaded_tree, sample)\n",
    "print(\"\\nSample:\", sample)\n",
    "print(\"Prediction from loaded tree:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ec273d-f3d7-42f4-ae6e-f00df9d70454",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
