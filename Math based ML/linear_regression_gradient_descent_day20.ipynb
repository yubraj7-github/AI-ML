{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2173959e-30cf-428c-865e-b24a26c825aa",
   "metadata": {},
   "source": [
    "# Gradient Descent: Intuition and Derivation\n",
    "\n",
    "## Intuition\n",
    "- Imagine you are **standing on a hill** and want to reach the **lowest point** (valley).  \n",
    "- You **look around** to see which direction goes **downhill the fastest**.  \n",
    "- You **take a small step** in that direction, then repeat.  \n",
    "- Eventually, you reach the bottom — this is how gradient descent finds the **minimum of a function**.  \n",
    "\n",
    "> In machine learning, the “hill” is the **loss function**, which measures how bad our model is. We want to **minimize** it.\n",
    "\n",
    "\n",
    "## Derivation \n",
    "1. Suppose our loss function is $L(w)$ and $w$ are the model parameters.  \n",
    "2. The **gradient** $\\frac{dL}{dw}$ tells us the **slope** of the function at $w$.  \n",
    "   - Positive slope → go left  \n",
    "   - Negative slope → go right  \n",
    "3. **Update rule**:  \n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\eta \\cdot \\frac{dL}{dw}\n",
    "$$\n",
    "\n",
    "   - $\\eta$ = **learning rate** (step size)  \n",
    "   - Minus sign ensures we go **downhill**.  \n",
    "4. Repeat until the slope is almost zero → we’ve reached the minimum.\n",
    "\n",
    "\n",
    "\n",
    "## Summary\n",
    "- **Gradient** = slope of loss  \n",
    "- **Step** = move opposite to slope  \n",
    "- Repeat → reach **minimum**  \n",
    "- **Learning rate** controls **step size**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8a6db3-a37a-4e2b-ad5f-bba8d00ed24e",
   "metadata": {},
   "source": [
    "# Cost Function for Linear Regression (MSE)\n",
    "\n",
    "**Definition:**  \n",
    "- A **cost function** measures **how wrong the model is**.  \n",
    "- For linear regression, we use **Mean Squared Error (MSE)**.\n",
    "\n",
    "\n",
    "\n",
    "**Formula:**  \n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $n$ = number of data points  \n",
    "- $y_i$ = actual value  \n",
    "- $\\hat{y}_i$ = predicted value  \n",
    "\n",
    "\n",
    "**Explanation in simple words:**  \n",
    "1. Subtract the predicted value from the actual value → gives **error**.  \n",
    "2. Square the error → makes it **positive** and penalizes large errors.  \n",
    "3. Take the **average** of all squared errors → this is the **MSE**.  \n",
    "\n",
    "> Lower MSE → better the model.  \n",
    "> Higher MSE → model is not accurate.\n",
    "\n",
    "\n",
    "\n",
    "Key Points  \n",
    "- MSE is always ≥ 0.  \n",
    "- MSE = 0 → perfect prediction.  \n",
    "- Used in **gradient descent** to update model parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0debb3c-0e78-4af2-85ec-11937f3f7509",
   "metadata": {},
   "source": [
    "# Learning Rate and Convergence\n",
    "\n",
    "## Learning Rate (η)\n",
    "- It is the **step size** used in gradient descent.  \n",
    "- Determines **how much we update the model parameters** in each step.  \n",
    "\n",
    "**Effect of learning rate:**\n",
    "1. **Too small η** → very slow learning, takes many steps to reach minimum.  \n",
    "2. **Too large η** → may **overshoot** the minimum or even diverge (never settle).  \n",
    "3. **Right η** → model **converges** to the minimum efficiently.  \n",
    "\n",
    "\n",
    "\n",
    "## Convergence\n",
    "- Convergence means the model **reaches the lowest point** of the cost function.  \n",
    "- When gradient descent **stops changing the parameters much**, it has converged.  \n",
    "- Good convergence → low cost (MSE) and stable parameters.  \n",
    "\n",
    "\n",
    "**Tips:**\n",
    "- Learning rate controls **speed of convergence**.  \n",
    "- If the cost function keeps **bouncing or increasing**, learning rate is too high.  \n",
    "- If the cost function decreases **very slowly**, learning rate is too low.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e9b7ec9-10f4-4789-89ca-64d8fad22cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values: [4.5, 7.5, 10.5]\n"
     ]
    }
   ],
   "source": [
    "# 1 Write a function predict(X, w, b) that computes Linear Regression predictions using loops\n",
    "# Synthetic dataset \n",
    "X = [\n",
    "    [1, 2],   # sample 1\n",
    "    [3, 4],   # sample 2\n",
    "    [5, 6]    # sample 3\n",
    "]\n",
    "\n",
    "# weights and bias\n",
    "w = [0.5, 1]  # weight for each feature\n",
    "b = 2         # bias\n",
    "\n",
    "# Function to predict\n",
    "def predict(X, w, b):\n",
    "    y_pred = []                \n",
    "    for sample in X:           \n",
    "        pred = 0              \n",
    "        for i in range(len(sample)):  \n",
    "            pred += sample[i] * w[i]  \n",
    "        pred += b              \n",
    "        y_pred.append(pred)   \n",
    "    return y_pred\n",
    "\n",
    "# Call function\n",
    "predictions = predict(X, w, b)\n",
    "print(\"Predicted values:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c575f770-76b8-4dda-af8b-fae71f06e538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.3125\n"
     ]
    }
   ],
   "source": [
    "# 2 Implement a function mse(y_true, y_pred) without using NumPy’s mean().\n",
    "\n",
    "# Example data\n",
    "y_true = [3, 5, 2, 7]\n",
    "y_pred = [2.5, 5, 4, 6]\n",
    "\n",
    "# Function to calculate MSE\n",
    "def mse(y_true, y_pred):\n",
    "    n = len(y_true)\n",
    "    squared_errors = []\n",
    "\n",
    "    for i in range(n):\n",
    "        squared_errors.append((y_true[i] - y_pred[i]) ** 2)\n",
    "\n",
    "    return sum(squared_errors) / n\n",
    "\n",
    "# Call function\n",
    "mse_result = mse(y_true, y_pred)\n",
    "print(\"Mean Squared Error:\", mse_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29ba43f8-d84f-4b0c-91d8-a164a047ccf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight: 0\n",
      "Bias: 0\n"
     ]
    }
   ],
   "source": [
    "# 3 Initialize weight and bias with zero and print their values.\n",
    "\n",
    "w = 0\n",
    "b = 0\n",
    "\n",
    "# Print values\n",
    "print(\"Weight:\", w)\n",
    "print(\"Bias:\", b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efe6d230-eea1-474f-9803-e93af88b63c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X: [1, 2, 3, 4, 5]\n",
      "Predicted y: [3, 5, 7, 9, 11]\n"
     ]
    }
   ],
   "source": [
    "# 4 Create a small numerical dataset and generate predictions using fixed w and b.\n",
    "# Input data (synthetic dataset)\n",
    "X = [1, 2, 3, 4, 5]\n",
    "\n",
    "w = 2   # weight\n",
    "b = 1   # bias\n",
    "\n",
    "# Predict values\n",
    "y_pred = []\n",
    "\n",
    "for x in X:\n",
    "    y = w * x + b\n",
    "    y_pred.append(y)\n",
    "\n",
    "print(\"Input X:\", X)\n",
    "print(\"Predicted y:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce46ae97-9c14-4582-ab0d-65c61d24e56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw: -22.666666666666664\n",
      "db: -10.0\n"
     ]
    }
   ],
   "source": [
    "# 5 Write compute_gradients(X, y, w, b) to calculate gradients dw and db.\n",
    "\n",
    "def compute_gradients(X, y, w, b):\n",
    "    n = len(X)\n",
    "    dw = 0\n",
    "    db = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        y_hat = w * X[i] + b\n",
    "        error = y_hat - y[i]\n",
    "        dw += error * X[i]\n",
    "        db += error\n",
    "\n",
    "    dw = (2 / n) * dw\n",
    "    db = (2 / n) * db\n",
    "\n",
    "    return dw, db\n",
    "    \n",
    "X = [1, 2, 3]\n",
    "y = [3, 5, 7]\n",
    "w = 0\n",
    "b = 0\n",
    "\n",
    "\n",
    "dw, db = compute_gradients(X, y, w, b)\n",
    "\n",
    "print(\"dw:\", dw)\n",
    "print(\"db:\", db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e14ce8b-ee4e-46a1-bf2e-78efbda24876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated w: 0.22666666666666666\n",
      "Updated b: 0.1\n"
     ]
    }
   ],
   "source": [
    "# 6 Implement one iteration of Gradient Descent to update w and b.\n",
    "# Given data\n",
    "X = [1, 2, 3]\n",
    "y = [3, 5, 7]\n",
    "\n",
    "# Initial parameters\n",
    "w = 0\n",
    "b = 0\n",
    "\n",
    "# Learning rate\n",
    "alpha = 0.01\n",
    "\n",
    "# Gradient function\n",
    "def compute_gradients(X, y, w, b):\n",
    "    n = len(X)\n",
    "    dw = 0\n",
    "    db = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        y_hat = w * X[i] + b\n",
    "        error = y_hat - y[i]\n",
    "        dw += error * X[i]\n",
    "        db += error\n",
    "\n",
    "    dw = (2 / n) * dw\n",
    "    db = (2 / n) * db\n",
    "\n",
    "    return dw, db\n",
    "\n",
    "# One iteration of Gradient Descent\n",
    "dw, db = compute_gradients(X, y, w, b)\n",
    "\n",
    "w = w - alpha * dw\n",
    "b = b - alpha * db\n",
    "\n",
    "print(\"Updated w:\", w)\n",
    "print(\"Updated b:\", b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6282d770-8138-42ce-86fa-6c333c7cb5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: w = 0.2267, b = 0.1000\n",
      "Epoch 2: w = 0.4282, b = 0.1889\n",
      "Epoch 3: w = 0.6073, b = 0.2680\n",
      "Epoch 4: w = 0.7666, b = 0.3384\n",
      "Epoch 5: w = 0.9082, b = 0.4009\n",
      "Epoch 6: w = 1.0340, b = 0.4566\n",
      "Epoch 7: w = 1.1459, b = 0.5061\n",
      "Epoch 8: w = 1.2454, b = 0.5501\n",
      "Epoch 9: w = 1.3338, b = 0.5893\n",
      "Epoch 10: w = 1.4124, b = 0.6242\n"
     ]
    }
   ],
   "source": [
    "#  7 Write a training loop that runs Gradient Descent for n_epochs.\n",
    "# Dataset\n",
    "X = [1, 2, 3]\n",
    "y = [3, 5, 7]\n",
    "\n",
    "# Initialize parameters\n",
    "w = 0\n",
    "b = 0\n",
    "\n",
    "# Learning rate and epochs\n",
    "alpha = 0.01\n",
    "n_epochs = 10\n",
    "\n",
    "# Gradient function\n",
    "def compute_gradients(X, y, w, b):\n",
    "    n = len(X)\n",
    "    dw = 0\n",
    "    db = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        y_hat = w * X[i] + b\n",
    "        error = y_hat - y[i]\n",
    "        dw += error * X[i]\n",
    "        db += error\n",
    "\n",
    "    dw = (2 / n) * dw\n",
    "    db = (2 / n) * db\n",
    "    return dw, db\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    dw, db = compute_gradients(X, y, w, b)\n",
    "\n",
    "    w = w - alpha * dw\n",
    "    b = b - alpha * db\n",
    "\n",
    "    print(f\"Epoch {epoch}: w = {w:.4f}, b = {b:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ac30427-7c1b-4b29-9792-69c3cbbbe2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Cost = 21.8693\n",
      "Epoch 2: Cost = 17.2868\n",
      "Epoch 3: Cost = 13.6646\n",
      "Epoch 4: Cost = 10.8014\n",
      "Epoch 5: Cost = 8.5382\n",
      "Epoch 6: Cost = 6.7493\n",
      "Epoch 7: Cost = 5.3353\n",
      "Epoch 8: Cost = 4.2175\n",
      "Epoch 9: Cost = 3.3340\n",
      "Epoch 10: Cost = 2.6356\n",
      "\n",
      "Cost values: [21.869318518518515, 17.286823371193417, 13.664604567574095, 10.801432872835433, 8.538247336894429, 6.749318783944975, 5.335265023438729, 4.217529747854396, 3.334018396607334, 2.6356485398280736]\n"
     ]
    }
   ],
   "source": [
    "# 8 Store cost values after each epoch in a list.\n",
    "# Dataset\n",
    "X = [1, 2, 3]\n",
    "y = [3, 5, 7]\n",
    "\n",
    "# Initialize parameters\n",
    "w = 0\n",
    "b = 0\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.01\n",
    "n_epochs = 10\n",
    "\n",
    "# Gradient function\n",
    "def compute_gradients(X, y, w, b):\n",
    "    n = len(X)\n",
    "    dw = 0\n",
    "    db = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        y_hat = w * X[i] + b\n",
    "        error = y_hat - y[i]\n",
    "        dw += error * X[i]\n",
    "        db += error\n",
    "\n",
    "    dw = (2 / n) * dw\n",
    "    db = (2 / n) * db\n",
    "    return dw, db\n",
    "\n",
    "# MSE function\n",
    "def mse(y_true, y_pred):\n",
    "    n = len(y_true)\n",
    "    total = 0\n",
    "    for i in range(n):\n",
    "        total += (y_true[i] - y_pred[i]) ** 2\n",
    "    return total / n\n",
    "\n",
    "# Training loop\n",
    "costs = []  # store cost values\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    dw, db = compute_gradients(X, y, w, b)\n",
    "\n",
    "    w = w - alpha * dw\n",
    "    b = b - alpha * db\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = []\n",
    "    for x in X:\n",
    "        y_pred.append(w * x + b)\n",
    "\n",
    "    # Compute cost\n",
    "    cost = mse(y, y_pred)\n",
    "    costs.append(cost)\n",
    "\n",
    "    print(f\"Epoch {epoch}: Cost = {cost:.4f}\")\n",
    "\n",
    "# Print all stored costs\n",
    "print(\"\\nCost values:\", costs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efc08e63-543f-4142-bfeb-750f7b653f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X: [10, 20, 30, 40, 50]\n",
      "Normalized X: [0.0, 0.25, 0.5, 0.75, 1.0]\n"
     ]
    }
   ],
   "source": [
    "#  9 Normalize input features before training.\n",
    "# Example dataset\n",
    "X = [10, 20, 30, 40, 50]\n",
    "\n",
    "# Min-Max normalization\n",
    "X_min = min(X)\n",
    "X_max = max(X)\n",
    "\n",
    "X_normalized = []\n",
    "for x in X:\n",
    "    x_norm = (x - X_min) / (X_max - X_min)\n",
    "    X_normalized.append(x_norm)\n",
    "\n",
    "print(\"Original X:\", X)\n",
    "print(\"Normalized X:\", X_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e2d8d69-b533-4378-ab2e-8d512fb40e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: w = [0.8200000000000001, 1.04], b = 0.22, Cost = 20.7712\n",
      "Epoch 2: w = [1.1304, 1.4336], b = 0.30319999999999997, Cost = 2.9756\n",
      "Epoch 3: w = [1.2479253333333333, 1.5825493333333334], b = 0.334624, Cost = 0.4264\n",
      "Epoch 4: w = [1.2924508444444445, 1.6389028977777778], b = 0.3464520533333333, Cost = 0.0612\n",
      "Epoch 5: w = [1.309347007525926, 1.6602107373037036], b = 0.35086372977777774, Cost = 0.0089\n",
      "Epoch 6: w = [1.3157857323741233, 1.6682545081204936], b = 0.3524687757463703, Cost = 0.0014\n",
      "Epoch 7: w = [1.3182662792267008, 1.6712781748660568], b = 0.35301189563935603, Cost = 0.0003\n",
      "Epoch 8: w = [1.3192485023747327, 1.672401929358415], b = 0.3531534269836823, Cost = 0.0002\n",
      "Epoch 9: w = [1.3196634135898058, 1.6728067075426574], b = 0.3531432939528515, Cost = 0.0002\n",
      "Epoch 10: w = [1.319863385235834, 1.6729394718908275], b = 0.3530760866549935, Cost = 0.0002\n"
     ]
    }
   ],
   "source": [
    "# 10 Extend your implementation to handle multiple input features.\n",
    "# 3 samples, 2 features\n",
    "X = [\n",
    "    [1, 2],   # sample 1\n",
    "    [3, 4],   # sample 2\n",
    "    [5, 6]    # sample 3\n",
    "]\n",
    "\n",
    "# Target values\n",
    "y = [5, 11, 17]\n",
    "\n",
    "# Initialize weights and bias\n",
    "w = [0, 0]   # one weight per feature\n",
    "b = 0\n",
    "\n",
    "def predict(X, w, b):\n",
    "    y_pred = []\n",
    "    for sample in X:\n",
    "        pred = 0\n",
    "        for i in range(len(sample)):\n",
    "            pred += sample[i] * w[i]\n",
    "        pred += b\n",
    "        y_pred.append(pred)\n",
    "    return y_pred\n",
    "    \n",
    "def compute_gradients(X, y, w, b):\n",
    "    n = len(X)\n",
    "    dw = [0] * len(w)\n",
    "    db = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        y_hat = sum([X[i][j] * w[j] for j in range(len(w))]) + b\n",
    "        error = y_hat - y[i]\n",
    "\n",
    "        for j in range(len(w)):\n",
    "            dw[j] += error * X[i][j]\n",
    "        db += error\n",
    "\n",
    "    dw = [(2 / n) * d for d in dw]\n",
    "    db = (2 / n) * db\n",
    "\n",
    "    return dw, db\n",
    "\n",
    "alpha = 0.01\n",
    "n_epochs = 10\n",
    "costs = []\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    dw, db = compute_gradients(X, y, w, b)\n",
    "\n",
    "    # Update weights and bias\n",
    "    for j in range(len(w)):\n",
    "        w[j] = w[j] - alpha * dw[j]\n",
    "    b = b - alpha * db\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = predict(X, w, b)\n",
    "\n",
    "    # Compute MSE\n",
    "    cost = sum((y[i] - y_pred[i])**2 for i in range(len(y))) / len(y)\n",
    "    costs.append(cost)\n",
    "\n",
    "    print(f\"Epoch {epoch}: w = {w}, b = {b}, Cost = {cost:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e82ad5f0-e866-4a3f-8cff-12b41b9f5fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Cost = 3943.7200, w = [8.200000000000001, 10.4], b = 2.2\n",
      "Epoch 2: Cost = 107261.6891, w = [-34.56, -43.84], b = -9.280000000000001\n",
      "Cost increased! Stopping training.\n"
     ]
    }
   ],
   "source": [
    "# 11 Add logic to stop training if the cost increases between iterations.\n",
    "# Dataset (3 samples, 2 features)\n",
    "X = [\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "]\n",
    "y = [5, 11, 17]\n",
    "\n",
    "# Initialize weights and bias\n",
    "w = [0, 0]\n",
    "b = 0\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "n_epochs = 50\n",
    "costs = []\n",
    "\n",
    "# Functions\n",
    "def predict(X, w, b):\n",
    "    y_pred = []\n",
    "    for sample in X:\n",
    "        pred = 0\n",
    "        for i in range(len(sample)):\n",
    "            pred += sample[i] * w[i]\n",
    "        pred += b\n",
    "        y_pred.append(pred)\n",
    "    return y_pred\n",
    "\n",
    "def compute_gradients(X, y, w, b):\n",
    "    n = len(X)\n",
    "    dw = [0 for _ in w]\n",
    "    db = 0\n",
    "    for i in range(n):\n",
    "        y_hat = 0\n",
    "        for j in range(len(w)):\n",
    "            y_hat += X[i][j] * w[j]\n",
    "        y_hat += b\n",
    "        error = y_hat - y[i]\n",
    "        for j in range(len(w)):\n",
    "            dw[j] += error * X[i][j]\n",
    "        db += error\n",
    "    for j in range(len(w)):\n",
    "        dw[j] = (2 / n) * dw[j]\n",
    "    db = (2 / n) * db\n",
    "    return dw, db\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    total = 0\n",
    "    for i in range(len(y_true)):\n",
    "        total += (y_true[i] - y_pred[i]) ** 2\n",
    "    return total / len(y_true)\n",
    "\n",
    "# Training loop with early stopping\n",
    "previous_cost = float('inf')\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    dw, db = compute_gradients(X, y, w, b)\n",
    "\n",
    "    # Update parameters\n",
    "    for j in range(len(w)):\n",
    "        w[j] -= alpha * dw[j]\n",
    "    b -= alpha * db\n",
    "\n",
    "    # Compute cost\n",
    "    y_pred = predict(X, w, b)\n",
    "    cost = mse(y, y_pred)\n",
    "    costs.append(cost)\n",
    "\n",
    "    print(f\"Epoch {epoch}: Cost = {cost:.4f}, w = {w}, b = {b}\")\n",
    "\n",
    "    # Early stopping if cost increases\n",
    "    if cost > previous_cost:\n",
    "        print(\"Cost increased! Stopping training.\")\n",
    "        break\n",
    "    previous_cost = cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f407e9f3-cc2f-486f-9c03-be51611f3591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Cost = 20.771200, w = [0.8200000000000001, 1.04], b = 0.22\n",
      "Epoch 2: Cost = 2.975592, w = [1.1304, 1.4336], b = 0.30319999999999997\n",
      "Epoch 3: Cost = 0.426394, w = [1.2479253333333333, 1.5825493333333334], b = 0.334624\n",
      "Epoch 4: Cost = 0.061223, w = [1.2924508444444445, 1.6389028977777778], b = 0.3464520533333333\n",
      "Epoch 5: Cost = 0.008912, w = [1.309347007525926, 1.6602107373037036], b = 0.35086372977777774\n",
      "Epoch 6: Cost = 0.001417, w = [1.3157857323741233, 1.6682545081204936], b = 0.3524687757463703\n",
      "Epoch 7: Cost = 0.000342, w = [1.3182662792267008, 1.6712781748660568], b = 0.35301189563935603\n",
      "Epoch 8: Cost = 0.000186, w = [1.3192485023747327, 1.672401929358415], b = 0.3531534269836823\n",
      "Improvement 0.000155 < epsilon (0.001), stopping training.\n"
     ]
    }
   ],
   "source": [
    "# 12 Implement early stopping when cost improvement is less than a small threshold.\n",
    "# Small dataset (2 features, 3 samples)\n",
    "X = [\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "]\n",
    "y = [5, 11, 17]\n",
    "\n",
    "# Initialize weights and bias\n",
    "w = [0, 0]\n",
    "b = 0\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.01\n",
    "n_epochs = 100\n",
    "epsilon = 0.001  # minimum improvement threshold\n",
    "costs = []\n",
    "\n",
    "# Prediction function\n",
    "def predict(X, w, b):\n",
    "    y_pred = []\n",
    "    for sample in X:\n",
    "        pred = 0\n",
    "        for i in range(len(sample)):\n",
    "            pred += sample[i] * w[i]\n",
    "        pred += b\n",
    "        y_pred.append(pred)\n",
    "    return y_pred\n",
    "\n",
    "# Gradient function\n",
    "def compute_gradients(X, y, w, b):\n",
    "    n = len(X)\n",
    "    dw = [0 for _ in w]\n",
    "    db = 0\n",
    "    for i in range(n):\n",
    "        y_hat = sum([X[i][j] * w[j] for j in range(len(w))]) + b\n",
    "        error = y_hat - y[i]\n",
    "        for j in range(len(w)):\n",
    "            dw[j] += error * X[i][j]\n",
    "        db += error\n",
    "    for j in range(len(w)):\n",
    "        dw[j] = (2 / n) * dw[j]\n",
    "    db = (2 / n) * db\n",
    "    return dw, db\n",
    "\n",
    "# MSE function\n",
    "def mse(y_true, y_pred):\n",
    "    return sum((y_true[i] - y_pred[i])**2 for i in range(len(y_true))) / len(y_true)\n",
    "\n",
    "# Training loop with early stopping by threshold\n",
    "previous_cost = float('inf')\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    dw, db = compute_gradients(X, y, w, b)\n",
    "\n",
    "    # Update parameters\n",
    "    for j in range(len(w)):\n",
    "        w[j] -= alpha * dw[j]\n",
    "    b -= alpha * db\n",
    "\n",
    "    # Compute cost\n",
    "    y_pred = predict(X, w, b)\n",
    "    cost = mse(y, y_pred)\n",
    "    costs.append(cost)\n",
    "\n",
    "    print(f\"Epoch {epoch}: Cost = {cost:.6f}, w = {w}, b = {b}\")\n",
    "\n",
    "    # Early stopping: if improvement is smaller than epsilon\n",
    "    improvement = previous_cost - cost\n",
    "    if improvement < epsilon:\n",
    "        print(f\"Improvement {improvement:.6f} < epsilon ({epsilon}), stopping training.\")\n",
    "        break\n",
    "\n",
    "    previous_cost = cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e805d74-fd7c-4d9b-bb10-cb6ec3423036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: w = [82.0, 104.0], b = 22.0, Cost = 542233.00\n",
      "Epoch 2: w = [-4932.0, -6256.0], b = -1324.0, Cost = 2027703307.67\n",
      "Epoch 3: w = [301683.3333333333, 382669.3333333333], b = 80986.0, Cost = 7582682544650.18\n",
      "Epoch 4: w = [-18448395.555555556, -23400814.22222222], b = -4952418.666666666, Cost = 28355763072215100.00\n",
      "Epoch 5: w = [1128152645.259259, 1431001973.037037], b = 302849327.77777773, Cost = 106037579006242177024.00\n",
      "Epoch 6: w = [-68988562837.87653, -87508343799.50616], b = -18519780961.629623, Cost = 396532025354755999006720.00\n",
      "Epoch 7: w = [4218774674016.534, 5351292582423.473], b = 1132517908406.9382, Cost = 1482848331747451711483019264.00\n",
      "Epoch 8: w = [-257985657587817.4, -327241164199689.3], b = -69255506611871.91, Cost = 5545174246642543779494460653568.00\n",
      "Epoch 9: w = [1.577628687565679e+16, 2.0011385641393104e+16], b = 4235098765736313.0, Cost = 20736414350206551133490024258994176.00\n",
      "Epoch 10: w = [-9.64748311631617e+17, -1.2237322167824387e+18], b = -2.5898390515082182e+17, Cost = 77544701208227127190834153072646684672.00\n"
     ]
    }
   ],
   "source": [
    "# Train the model using a very large learning rate and observe parameter behavior.\n",
    "# Small dataset (2 features, 3 samples)\n",
    "X = [\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "]\n",
    "y = [5, 11, 17]\n",
    "\n",
    "# Initialize weights and bias\n",
    "w = [0, 0]\n",
    "b = 0\n",
    "\n",
    "# Very large learning rate\n",
    "alpha = 1.0  # large!\n",
    "n_epochs = 10\n",
    "costs = []\n",
    "\n",
    "# Prediction function\n",
    "def predict(X, w, b):\n",
    "    y_pred = []\n",
    "    for sample in X:\n",
    "        pred = sum([sample[j]*w[j] for j in range(len(w))]) + b\n",
    "        y_pred.append(pred)\n",
    "    return y_pred\n",
    "\n",
    "# Gradient function\n",
    "def compute_gradients(X, y, w, b):\n",
    "    n = len(X)\n",
    "    dw = [0 for _ in w]\n",
    "    db = 0\n",
    "    for i in range(n):\n",
    "        y_hat = sum([X[i][j]*w[j] for j in range(len(w))]) + b\n",
    "        error = y_hat - y[i]\n",
    "        for j in range(len(w)):\n",
    "            dw[j] += error * X[i][j]\n",
    "        db += error\n",
    "    for j in range(len(w)):\n",
    "        dw[j] = (2/n) * dw[j]\n",
    "    db = (2/n) * db\n",
    "    return dw, db\n",
    "\n",
    "# MSE function\n",
    "def mse(y_true, y_pred):\n",
    "    return sum((y_true[i]-y_pred[i])**2 for i in range(len(y_true))) / len(y_true)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    dw, db = compute_gradients(X, y, w, b)\n",
    "\n",
    "    # Update parameters\n",
    "    for j in range(len(w)):\n",
    "        w[j] -= alpha * dw[j]\n",
    "    b -= alpha * db\n",
    "\n",
    "    # Compute cost\n",
    "    y_pred = predict(X, w, b)\n",
    "    cost = mse(y, y_pred)\n",
    "    costs.append(cost)\n",
    "\n",
    "    print(f\"Epoch {epoch}: w = {w}, b = {b}, Cost = {cost:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7d8329a-9403-4425-b6b8-39e0180263ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: w = [0.8200000000000001, 1.04], b = 0.22, Cost = 20.7712\n",
      "Epoch 2: w = [1.1304, 1.4336], b = 0.30319999999999997, Cost = 2.9756\n",
      "Epoch 3: w = [1.2479253333333333, 1.5825493333333334], b = 0.334624, Cost = 0.4264\n",
      "Epoch 4: w = [1.2924508444444445, 1.6389028977777778], b = 0.3464520533333333, Cost = 0.0612\n",
      "Epoch 5: w = [1.309347007525926, 1.6602107373037036], b = 0.35086372977777774, Cost = 0.0089\n",
      "Epoch 6: w = [1.3157857323741233, 1.6682545081204936], b = 0.3524687757463703, Cost = 0.0014\n",
      "Epoch 7: w = [1.3182662792267008, 1.6712781748660568], b = 0.35301189563935603, Cost = 0.0003\n",
      "Epoch 8: w = [1.3192485023747327, 1.672401929358415], b = 0.3531534269836823, Cost = 0.0002\n",
      "Epoch 9: w = [1.3196634135898058, 1.6728067075426574], b = 0.3531432939528515, Cost = 0.0002\n",
      "Epoch 10: w = [1.319863385235834, 1.6729394718908275], b = 0.3530760866549935, Cost = 0.0002\n",
      "Epoch 11: w = [1.319981785060197, 1.6729693891166744], b = 0.3529876040564774, Cost = 0.0002\n",
      "Epoch 12: w = [1.3200690914952047, 1.6729604852376068], b = 0.3528913937424021, Cost = 0.0002\n",
      "Epoch 13: w = [1.3201444108520815, 1.6729369924109145], b = 0.3527925815588332, Cost = 0.0002\n",
      "Epoch 14: w = [1.3202149756525308, 1.6729080815361892], b = 0.3526931058836585, Cost = 0.0002\n",
      "Epoch 15: w = [1.3202835243966384, 1.6728772231005766], b = 0.3525936987039383, Cost = 0.0001\n",
      "Epoch 16: w = [1.3203510946723507, 1.6728457300903659], b = 0.3524946354180151, Cost = 0.0001\n",
      "Epoch 17: w = [1.3204180802972139, 1.6728140989192986], b = 0.3523960186220845, Cost = 0.0001\n",
      "Epoch 18: w = [1.3204846314275447, 1.672782516945811], b = 0.3522978855182661, Cost = 0.0001\n",
      "Epoch 19: w = [1.320550805992584, 1.6727510545591673], b = 0.3522002485665832, Cost = 0.0001\n",
      "Epoch 20: w = [1.3206166270096302, 1.6727197378805936], b = 0.35210311087096313, Cost = 0.0001\n"
     ]
    }
   ],
   "source": [
    "# 14 Implement Batch Gradient Descent without using NumPy.\n",
    "# Dataset (3 samples, 2 features)\n",
    "X = [\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "]\n",
    "y = [5, 11, 17]\n",
    "\n",
    "# Initialize parameters\n",
    "w = [0, 0]   # weights for 2 features\n",
    "b = 0        # bias\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.01\n",
    "n_epochs = 20\n",
    "\n",
    "# Prediction function\n",
    "def predict(X, w, b):\n",
    "    y_pred = []\n",
    "    for sample in X:\n",
    "        pred = 0\n",
    "        for i in range(len(w)):\n",
    "            pred += sample[i] * w[i]\n",
    "        pred += b\n",
    "        y_pred.append(pred)\n",
    "    return y_pred\n",
    "\n",
    "# Gradient computation (Batch)\n",
    "def compute_gradients(X, y, w, b):\n",
    "    n = len(X)\n",
    "    dw = [0 for _ in w]\n",
    "    db = 0\n",
    "    for i in range(n):\n",
    "        y_hat = 0\n",
    "        for j in range(len(w)):\n",
    "            y_hat += X[i][j] * w[j]\n",
    "        y_hat += b\n",
    "        error = y_hat - y[i]\n",
    "        for j in range(len(w)):\n",
    "            dw[j] += error * X[i][j]\n",
    "        db += error\n",
    "    for j in range(len(w)):\n",
    "        dw[j] = (2 / n) * dw[j]\n",
    "    db = (2 / n) * db\n",
    "    return dw, db\n",
    "\n",
    "# MSE function\n",
    "def mse(y_true, y_pred):\n",
    "    total = 0\n",
    "    for i in range(len(y_true)):\n",
    "        total += (y_true[i] - y_pred[i]) ** 2\n",
    "    return total / len(y_true)\n",
    "\n",
    "# Training loop (Batch Gradient Descent)\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # Compute gradients using all data points\n",
    "    dw, db = compute_gradients(X, y, w, b)\n",
    "\n",
    "    # Update weights and bias\n",
    "    for j in range(len(w)):\n",
    "        w[j] -= alpha * dw[j]\n",
    "    b -= alpha * db\n",
    "\n",
    "    # Compute current cost\n",
    "    y_pred = predict(X, w, b)\n",
    "    costs = mse(y, y_pred)\n",
    "    cost.append(costs)\n",
    "\n",
    "    print(f\"Epoch {epoch}: w = {w}, b = {b}, Cost = {costs:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8df9c276-98f5-43dd-9757-5de06eac8abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 15 Plot cost vs epochs and save the plot as gd_loss.png.\n",
    "import matplotlib.pyplot as plt\n",
    "costs = [70.6667, 33.9947, 16.3500, 7.9450, 3.8472, 1.9020, 1.2276, 0.7940, 0.5150, 0.3355]\n",
    "\n",
    "\n",
    "epochs = list(range(1, len(costs) + 1))\n",
    "# Create the plot\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(epochs, costs, marker='o', color='blue', label='MSE Cost')\n",
    "plt.title('Cost vs Epochs (Gradient Descent)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cost (MSE)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot as 'gd_loss.png'\n",
    "plt.savefig('gd_loss.png')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e9220b-fdfa-4ffa-b074-b2c1543c2bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
